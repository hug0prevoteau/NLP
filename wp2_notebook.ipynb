{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\hugop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\hugop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_neg = os.listdir(\"data/neg/\")\n",
    "all_files_pos = os.listdir(\"data/pos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg=[]\n",
    "for fichier in all_files_neg:\n",
    "    neg.append(open(f\"data/neg/{fichier}\", 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]\n",
    "for fichier in all_files_pos:\n",
    "    pos.append(open(f\"data/pos/{fichier}\", 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neg)):\n",
    "    neg[i] = english_tokenizer.tokenize(neg[i])\n",
    "for i in range(len(pos)):\n",
    "    pos[i] = english_tokenizer.tokenize(pos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neg)):\n",
    "    for j in range(len(neg[i])):\n",
    "        neg[i][j] = tokenizer.tokenize(neg[i][j])\n",
    "for i in range(len(pos)):\n",
    "    for j in range(len(pos[i])):\n",
    "        pos[i][j] = tokenizer.tokenize(pos[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neg)):\n",
    "    for j in range(len(neg[i])):\n",
    "        neg[i][j] = tagger.tag(neg[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos)):\n",
    "    for j in range(len(pos[i])):\n",
    "        pos[i][j] = tagger.tag(pos[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting only adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neg)):\n",
    "    for j in range(len(neg[i])):\n",
    "        tmp = []\n",
    "        for x in range(len(neg[i][j])):\n",
    "            if neg[i][j][x][1] == 'RB' or neg[i][j][x][1] == 'RBR' or neg[i][j][x][1] == 'RBS':\n",
    "                tmp.append(neg[i][j][x][0])\n",
    "        neg[i][j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos)):\n",
    "    for j in range(len(pos[i])):\n",
    "        tmp = []\n",
    "        for x in range(len(pos[i][j])):\n",
    "            if pos[i][j][x][1] == 'RB' or pos[i][j][x][1] == 'RBR' or pos[i][j][x][1] == 'RBS':\n",
    "                tmp.append(pos[i][j][x][0])\n",
    "        pos[i][j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neg)):\n",
    "    tmp = []\n",
    "    for j in range(len(neg[i])):\n",
    "        tmp += neg[i][j]\n",
    "    neg[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos)):\n",
    "    tmp = []\n",
    "    for j in range(len(pos[i])):\n",
    "        tmp += pos[i][j]\n",
    "    pos[i] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_words(_list):\n",
    "    total_list_pos = []\n",
    "    total_list_obj = []\n",
    "    total_list_neg = []\n",
    "    for i in range(len(_list)):\n",
    "        list_pos = []\n",
    "        list_obj = []\n",
    "        list_neg = []\n",
    "        for j in range(len(_list[i])):\n",
    "            try:\n",
    "                if ((list(swn.senti_synsets(_list[i][j]))[0].pos_score()) != 0.0):\n",
    "                    list_pos.append(list(swn.senti_synsets(_list[i][j]))[0].pos_score())\n",
    "                if ((list(swn.senti_synsets(_list[i][j]))[0].neg_score()) != 0.0):\n",
    "                    list_neg.append(list(swn.senti_synsets(_list[i][j]))[0].neg_score())\n",
    "                if ((list(swn.senti_synsets(_list[i][j]))[0].obj_score()) != 0.0):\n",
    "                    list_obj.append(list(swn.senti_synsets(_list[i][j]))[0].obj_score())\n",
    "\n",
    "            except IndexError:\n",
    "                continue\n",
    "        if len(list_pos) == 0:\n",
    "            list_pos = 0\n",
    "        if len(list_neg) == 0:\n",
    "            list_neg = 0\n",
    "        if len(list_obj) == 0:\n",
    "            list_obj = 0\n",
    "        total_list_pos.append(np.mean(list_pos))\n",
    "        total_list_neg.append(np.mean(list_neg))\n",
    "        total_list_obj.append(np.mean(list_obj))\n",
    "    return total_list_pos ,total_list_neg, total_list_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pos, neg_neg, neg_obj = score_words(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pos, pos_neg, pos_obj = score_words(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pos = neg_pos + pos_pos\n",
    "score_neg = neg_neg + pos_neg\n",
    "score_obj = neg_obj + pos_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe = [0]*1000 + [1]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(score_pos, score_neg, score_obj)), columns =['pos','neg','obj']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=2, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=KMeans(n_clusters=2)\n",
    "model.fit(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = model.predict(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.43      0.48      1000\n",
      "           1       0.52      0.62      0.57      1000\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.53      0.53      0.52      2000\n",
      "weighted avg       0.53      0.53      0.52      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(classe, clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
