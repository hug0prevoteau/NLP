{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: **Christophe HAIKAL and Hugo PREVOTEAU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menu\n",
    "**Our work is divided in the following parts: <br>**\n",
    "[Imports](#Imports): importing libraries and data<br>\n",
    "[Preprocessing](#Preprocessing): scrapping the XML file to create our dataframe\n",
    "\n",
    "**Our two tested approaches: <br>**\n",
    "[TF-IDF](#TF-IDF)<br>\n",
    "[Doc2Vec](#Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we decided to use two different approaches, the first one would be the TF-IDF (the one from the first project) as well as a Doc2Vec model (that we also used in the first project). These two approaches provide vector representations for each document, and allows us to perform classification as well as regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"C:/Users/hugop/Desktop/corpus-taln/corpus_taln_v1.tei.xml\",\"r\", encoding='UTF-8')\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents,'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.bibl.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We use the XML tags to retrieve information that we want, using beaufitul soup, since XML and HTML work the same way*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*authors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = soup.find_all('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = {'xml:lang':[\"fr\",\"en\"]}\n",
    "titles = soup.find_all('title', selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "for x in titles:\n",
    "    title.append(x.text.replace('\\n',''))\n",
    "title = list(dict.fromkeys(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_list = []\n",
    "for x in titles:\n",
    "    titles_list.append(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_list = list(dict.fromkeys(titles_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*languages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = []\n",
    "for x in titles_list:\n",
    "    language.append(re.search('\\\"(.*)\\\"', x).group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*texts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for x in author:\n",
    "    paragraphs.append(BeautifulSoup(str(x), \"lxml\").text.replace('\\n\\n\\n\\n\\n',' ').replace('\\n\\n\\n\\n',' ').replace('\\n\\n\\n',' ').replace('\\n\\n',' ').replace('\\n',' ').replace(\"\\'\",\"\").replace('None ','').replace('  ',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_clean = []\n",
    "regex = re.compile(\".?((.?))\")\n",
    "for x in paragraphs:\n",
    "    paragraphs_clean.append(re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", x).replace('[]', '').replace('()', '').replace('  ', ' ').replace(' .','.'))\n",
    "paragraphs_clean = list(dict.fromkeys(paragraphs_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*dates*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = soup.find_all('imprint') #we use this tag because its unique in each article, avoids duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*conference*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = soup.find_all('fileDesc', {\"xml:id\": re.compile('^r|t')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = conf[1:] ##supprimer le fileDesc qui correspond à la description du fichier xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences = []\n",
    "for x in conf:\n",
    "    conferences.append(str(x).replace('-', '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences_clean = []\n",
    "for x in conferences:\n",
    "    conferences_clean.append(re.search('\\\"(.*)\\\"', x).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences_clean_2 = []\n",
    "for x in conferences_clean:\n",
    "    if x[0] == 't':\n",
    "        x = x[0:4]\n",
    "    else:\n",
    "        x = x[0:7]\n",
    "    conferences_clean_2.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "for x in dates:\n",
    "    date.append(x.get_text().replace('\\n',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once we scrapped all the data we can zip the lists into a single pandas dataframe that we'll use later*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling a dataframe with the scrapped data\n",
    "df = pd.DataFrame(list(zip(paragraphs_clean, language,title, date,conferences_clean_2)), columns =['Text', 'Language','name', 'date', 'conference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr = df[df['Language'] == 'fr'] #we focus on french papers only for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr = df_fr[df_fr['Text'].str.len() > 2570] #we delete the articles with no text body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'taln': 1089, 'recital': 196})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(df_fr['conference']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nous considérons dans notre travail la tâche d...\n",
       "1    nous donnons ici un aperçu du logiciel decid d...\n",
       "3    le terme de lambda-drt désigne un ensemble de ...\n",
       "4    dans cet article, nous comparons deux modèles ...\n",
       "5    dans le cadre des approches à base de grammair...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fr['Text'] = df_fr['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df_fr['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    considérons travail tâche traitement automatiq...\n",
       "1    donnons ici aperçu logiciel decid développé ge...\n",
       "3    terme lambda-drt désigne ensemble méthodes per...\n",
       "4    cet article, comparons deux modèles linguistiq...\n",
       "5    cadre approches base grammaires faiblement sen...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('french')\n",
    "df_fr['Text'] = df_fr['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df_fr['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr['Text'] = df_fr['Text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We didn't use this, since we think that the french lemmatizer is not good enough*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "lemmatizerFR = FrenchLefffLemmatizer()\n",
    "df_fr['Text'] = df_fr.apply(lambda x: \" \".join([lemmatizerFR.lemmatize(word) for word in x['Text'].split()]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr['Text_no_stemming'] = df_fr['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmerFR = FrenchStemmer()\n",
    "df_fr['Text'] = df_fr.apply(lambda x: \" \".join([stemmerFR.stem(word) for word in x['Text'].split()]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    considéron travail tâch trait automat vis cons...\n",
       "1    donnon ici aperçu logiciel decid développ get ...\n",
       "3    term lambdadrt désign ensembl méthod permet co...\n",
       "4    cet articl comparon deux model linguist utilis...\n",
       "5    cadr approch bas grammair faibl sensibl contex...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fr['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountFrequency(my_list): \n",
    "  \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "  \n",
    "    for key, value in freq.items(): \n",
    "        print (\"% d : % d\"%(key, value)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1997 :  2\n",
      " 1998 :  12\n",
      " 1999 :  31\n",
      " 2000 :  13\n",
      " 2001 :  29\n",
      " 2002 :  47\n",
      " 2003 :  45\n",
      " 2004 :  65\n",
      " 2005 :  66\n",
      " 2006 :  63\n",
      " 2007 :  81\n",
      " 2008 :  62\n",
      " 2009 :  96\n",
      " 2010 :  85\n",
      " 2011 :  95\n",
      " 2012 :  69\n",
      " 2013 :  59\n",
      " 2014 :  86\n",
      " 2015 :  78\n",
      " 2016 :  45\n",
      " 2017 :  52\n",
      " 2018 :  68\n",
      " 2019 :  36\n"
     ]
    }
   ],
   "source": [
    "CountFrequency(sorted(pd.to_numeric(df_fr['date'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the TD-IDF, that will provide vectors for the documents. One of the main pros of TF-IDF is that it automatically deletes the stopwords relative to the papers themselves (for example talking about NLP, the word \"corpus\" may be a stopword, since it appears in almost all of the documents). On the other hand the french stopwords from the NLTK library are not as great as the english ones, one more reason to rely on the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df_fr = shuffle(df_fr, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.41 s\n",
      "(1285, 849)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2,\n",
    "                                 use_idf=True, ngram_range=(1,1), stop_words=['proceedings','proceeding'])\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(list(df_fr['Text'])) #fit the vectorizer to reviews\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tfidf_matrix.toarray()\n",
    "a = a.tolist()\n",
    "\n",
    "X_train = a[:1000]\n",
    "y_train = list(df_fr['date'])[:1000]\n",
    "X_test = a[1000:]\n",
    "y_test = list(df_fr['date'])[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for item in y_train:\n",
    "    a.append(float(item))\n",
    "    \n",
    "y_train = a\n",
    "\n",
    "\n",
    "a = []\n",
    "for item in y_test:\n",
    "    a.append(float(item))\n",
    "    \n",
    "y_test = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier (as baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Dummy Classifier as a beseline for our classification methods. The dummy classifier is a naive method that always predicts the most frequent class (hence, maximizing the performance of a naive classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can see that our Dummy Classifier gets 7.36% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07368421052631578"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try using the KNN for classification, a very simple algorithm that usually performs fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Setup arrays to store training and test accuracies\n",
    "neighbors = np.arange(1,40,2)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXhU5dn48e+dnZAVEtYAQYuygxDBKlYQd62KC0rVqrVVW7fq5Vtt1b60tu+vtfW1+rZK1VrUKrhVay3VShWtK+Auq4gBwhr2LGS/f388Z4aTYZJMQoYZMvfnus41c/Z7TuDc5zzPeZ4jqooxxpjElRTrAIwxxsSWJQJjjElwlgiMMSbBWSIwxpgEZ4nAGGMSnCUCY4xJcJYIDjIiUioiJ8Q6jgNFRJaIyORYx7E/ROQyEXkrhvv/vohsFpFKEenZidsd6G0zOYJli0VERSSlhfkzReQvnRWbaR9LBF2Y959LReR837QUb1qxNz7bG5/gW+ZrIhK2gYmIvCIiPw8z/SwR2dTSf/SOUtURqrqgM7eZSEQkFfhf4CRVzVLVbZ21bVVd622zsbO2aWLDEkHXtx34eRtXbduBX0S4vdnAJSIiIdMvAZ5Q1YZIA+vspJEIOnDMegMZwJIohBOXIrlDMc1ZIjiIichQEflKRC5sZbGXgTrg4laWeRQYLSLHRbDbF4AewLG+OPKBM4DHRGSCiLwrIjtFZKOI/F5E0nzLqohcIyJfAF+IyB9E5O6Q3/V3Efmh9z1YFObd4TwtIo+JSIVXbFTiW2+ciHzkzXtGRJ4SkbAJLlBcIyK/FZEd3nE81Te/WRGcv+jCV8xxuYis89a/WkSOFJFPvd/++313Kf8nIrtEZLmITPXNyBWRP3nHa72I/CJwMvPifFtE7hGR7cDMML8lXUR+JyIbvOF33rTDgBXeYjtF5LUw6wZ+y6UislZEtorIbb75SSJyq4h8KSLbvOPfI2TdFG98sIi86R3/+d7fNrS456Jw+/FkeH+zChH5UETG+OIYJiILvGO7RETO9M2bLSIPiMg8EakCpojIaSKy1NvWehG5OfS3Gx9VteEgGoBS4ARgHLAWOKOVZWcCfwHOBFYDqUAKoECxt8xs3N3A9cBb3rSvuX8aLW73IeBh3/hVwMfe9/HAUd5+ioFlwA99yyrwKi6ZdAMmABuAJG9+AVAN9Pb/Xt/vqQFOA5KB/we8581LA9YAN3i/8xxcAvxFC7/hMqAe+J63re97cUjofv3H0vte7P2OWbir7ZO8uF4AegH9gS3Acb59NQA3erFdAOwCenjzXwD+CHT31l8IXBWy7nXeMe0W5rf8HHjPW7cQeAe4MyTWlBaOQ2D+Q97fYwxQCwzz5v/Q23YRkO7FOSfctoF3gd96f4tJwO4wx6yl/cz0/h7necfoZuAr73sqsAr4ibft44EK4HDfv+FdwDG4i9sMYCNwrDc/HxgX6/+78TzEPAAb2vkHcyeonwFlwJQ2lvWfvN7HnexaSgTpuMRyKm0ngknef7xu3vjbwI0tLPtD4HnfuALHhyyzDDjR+34tMC/k9/oTwXzfvOHAHu/7N4D1eCdyb9pbtJ4IVvnGM73Y+oTuN8yxDJzU+vvmbwMu8I0/h5cAvX0Fk4w3bSGuOK23d0Ls5ps3A3jdt+7aNv7OXwKn+cZPBkpDYm0rERSFxHah728z1TevL+6EneLfNjAQl7Ayfcv+Jcwxa2k/M/GSujeehHcy94ZNeBcL3vw5wEzfv+HHQn7XWtwFSk6s/88eDIMVDR2crgbeUdXXAxNE5CJxT3BUisg/w6xzO3Ab7mppH6paC9zpDaHl/6HLvgWUA2eJyCHAkcCTXhyHichL4iqOdwP/g7vK91sXMv4oe4uuLgYeb2X3m3zfq3HFCSlAP2C9emeBFvbT4rZUtdr7mtXGOn6bfd/3hBn3bys0tjW4mAfhrng3esUeO3FX3b18y7b1O/p52wvddnuEHtdA7IOA532xLQMacQksNIbtvuPYUtwt7afZ8qrahLvY6ecN67xpAWtwd14t7etc3J3jGhF5Q0S+HiYW47FEcHC6GhgoIvcEJqjqE+qe4MhS1VNDV1DVV3G31z9oZbt/BnKBaRHE8BjwbdxV7b9UNXASfABYDgxR1Rzc7XxoYgl9IukvuKQyBhiGKyppr41Af5FmldgDOrCdgCrcXUJAn/3YFuwb20DcXcI63B1BgarmeUOOqo7wLdtWF8EbcCfs0G13hnXAqb7Y8lQ1Q1XXhyy3EeghIv5j1t7jH1xeRJJwxVEbvGGANy1gIO4OMKDZMVLVRap6Fi6hvgA83c5YEoolgoNTBXAK8A0R+VU71rsN+FFLM9U98TMTuCWCbT2Gq6v4Hu6KPiAbVzZcKSJDccVRrVLVMmAR7k7gOVXdE8H+Q72Lu1K9Vtwjsmfh6h866mPgQhFJ9Sqkz9uPbYE7IV3vbe98XMKbp6obgX8Bd4tIjlc5e6hEVnEfMAe4XUQKRaQA+CkuuXaGWcAvRWQQgLePs0IXUtU1wGJgpoikeVfg32znvsaLyDneHd4PcQnyPVyxZhXwI+/4Tfa2PTfcRrz9XyQiuapaj/v3aI+4tsISwUFKVXcCJwKnisidEa7zNq5ctjVzcFd3bW2rFFcp2R140TfrZuBbuGT1EPBUJLHhkskoWi8Wai2eOlwF8RXATlwR00u4k0lH3AEcCuzA1ck82cHtBLwPDAG2Ar8EztO9z/R/G1cJutTb37O4svhI/QJ3Ev4U+Az4kMgfB27Lvbi/779EpAJ3Yp7YwrIXAV/H1Zf8Ave3b8/x/xuuIn0H7k7zHFWt9/62Z+Lqr7YC9wPfVtXlrWzrEqDUK568mtafmkt4gSckjIkpEfkG7iq2OKQseH+2+T4wS1X/3BnbM+0jIk8By1X1v2Mdi2md3RGYmBPX+vUG3COpHU4CInKciPTxioYuBUbj2lGYA0BcO4pDveKtU4Cz6Fh9jznArGWniSkRGYYr1vgEuHw/N3c4rlIwC/dI5XleGbw5MPoAfwV64p74+b6qfhTbkEwkrGjIGGMSnBUNGWNMgjvoioYKCgq0uLg41mEYY8xB5YMPPtiqqoXh5h10iaC4uJjFixfHOgxjjDmoiMialuZZ0ZAxxiQ4SwTGGJPgLBEYY0yCO+jqCIwx7VdfX09ZWRk1NTWxDsVEWUZGBkVFRaSmpka8jiUCYxJAWVkZ2dnZFBcXI/u8ZdR0FarKtm3bKCsrY/DgwRGvF7WiIRF5RES2iMjnLcwXEblPRFaJe73fuGjFYkyiq6mpoWfPnpYEujgRoWfPnu2+84tmHcFsXFfJLTkV1xvjEOBKXD/2xpgosSSQGDryd45aIlDVN4HtrSxyFu71cqqq7wF5ItKernfbZ/MSmD8T9uyM2i6MMeZgFMunhvrT/PVyZTR/9VyQiFwpIotFZHF5eXnH9rajFN66B7Z/2bH1jTEdtm3bNsaOHcvYsWPp06cP/fv3D47X1dW1uf6CBQt45513guOzZs3iscce67T4ysvLSU1N5Y9//GOnbfNgEsvK4nD3L2F7wFPVB4EHAUpKSjrWS16e9ya/HaXQf3yHNmGM6ZiePXvy8ccfAzBz5kyysrK4+eabI15/wYIFZGVlcfTRRwNw9dVXd2p8zzzzDEcddRRz5szhqquu6tRt+zU0NJCSEn/P6MTyjqCM5u80DbyfNDryA4mgxVbWxpgD6IMPPuC4445j/PjxnHzyyWzc6HoMv++++xg+fDijR4/mwgsvpLS0lFmzZnHPPfcwduxY/vOf/zBz5kx++9vfAjB58mRuueUWJkyYwGGHHcZ//vMfAKqrq5k+fTqjR4/mggsuYOLEiS12TzNnzhzuvvtuysrKWL9+76uQH3vsMUaPHs2YMWO45JJLANi8eTPTpk1jzJgxjBkzhnfeeYfS0lJGjhwZXO+3v/0tM2fODMb3k5/8hOOOO457772Xv//970ycOJEjjjiCE044gc2b3eu+Kysrufzyyxk1ahSjR4/mueee409/+hM33nhjcLsPPfQQN910Uyf9BfaKZWp6Efd+2bm4V9/timrf8enZkNkTdloiMIntZ39fwtINuzt1m8P75fDf3xwR8fKqynXXXcff/vY3CgsLeeqpp7jtttt45JFH+NWvfsVXX31Feno6O3fuJC8vj6uvvrrZXcS///3vZttraGhg4cKFzJs3j5/97GfMnz+f+++/n/z8fD799FM+//xzxo4dGzaWdevWsWnTJiZMmMD06dN56qmnuOmmm1iyZAm//OUvefvttykoKGD7dlflef3113Pcccfx/PPP09jYSGVlJTt27Gj19+7cuZM33ngDgB07dvDee+8hIjz88MPcdddd3H333dx5553k5uby2WefBZdLS0tj9OjR3HXXXaSmpvLnP/85KsVXUUsEIjIHmAwUiEgZ8N9AKoCqzgLmAacBq4Bq9v+lJG3LG2R3BMbEgdraWj7//HNOPPFEABobG+nb1z0rMnr0aC666CLOPvtszj777Ii2d8455wAwfvx4SktLAXjrrbe44YYbABg5ciSjR48Ou+7cuXOZPn06ABdeeCFXXHEFN910E6+99hrnnXceBQUFAPTo0QOA1157LVg/kZycTG5ubpuJ4IILLgh+Lysr44ILLmDjxo3U1dUFn/efP38+c+fODS6Xn58PwPHHH89LL73EsGHDqK+vZ9SoUREdk/aIWiJQ1RltzFfgmmjtP6z8QbDh4wO6S2PiTXuu3KNFVRkxYgTvvvvuPvP+8Y9/8Oabb/Liiy9y5513smTJkja3l56eDrgTc0NDQ3AfkZgzZw6bN2/miSeeAGDDhg188cUXqGrEj2KmpKTQ1LT3Lauhz/F37949+P26667jpptu4swzz2TBggXBIqSW9vfd736X//mf/2Ho0KFcfnl0rpcTq6+h/GLYVQZNjbGOxJiElp6eTnl5eTAR1NfXs2TJEpqamli3bh1TpkzhrrvuYufOnVRWVpKdnU1FRUW79jFp0iSefvppAJYuXRoscvFbsWIFVVVVrF+/ntLSUkpLS/nxj3/M3LlzmTp1Kk8//TTbtm0DCBYNTZ06lQcecM2eGhsb2b17N71792bLli1s27aN2tpaXnrppRbj2rVrF/37uwckH3300eD0k046id///vfB8cBdxsSJE1m3bh1PPvkkM2a0en3dYYmVCPIGQVM97I5enbQxpm1JSUk8++yz3HLLLYwZM4axY8fyzjvv0NjYyMUXX8yoUaM44ogjuPHGG8nLy+Ob3/wmzz//fLCyOBI/+MEPKC8vZ/To0fz6179m9OjR5ObmNltmzpw5TJs2rdm0c889lzlz5jBixAhuu+02jjvuOMaMGROspL333nt5/fXXGTVqFOPHj2fJkiWkpqby05/+lIkTJ3LGGWcwdOjQFuOaOXMm559/Pscee2yw2Ang9ttvZ8eOHYwcOZIxY8bw+uuvB+dNnz6dY445Jlhc1NkOuncWl5SUaIdfTPPla/D4NLjsH1A8qXMDMyaOLVu2jGHDhsU6jAOqsbGR+vp6MjIy+PLLL5k6dSorV64kLS0t1qG12xlnnMGNN97I1KlTI1o+3N9bRD5Q1ZJwy8ffA63R5G9LYInAmC6turqaKVOmUF9fj6rywAMPHHRJYOfOnUyYMIExY8ZEnAQ6IrESQe4AkCR7csiYBJCdnX3Qv9Y2Ly+PlStXRn0/iVVHkJIGOf2tLYExxvgkViIAa0tgjDEhEi8R5A9ydQTGGGOAhEwExVC5Cer3xDoSY4yJC4mXCAJPDu1c1/pyxphOsz/dUC9evJjrr7++zX0EeiY17ZdYTw3B3l5Id66BwsNiG4sxCaKtbqhb6565pKSEkpKwj783439fwcGisbGR5OTkWIeRwHcEVk9gTExddtll3HTTTUyZMoVbbrmFhQsXcvTRR3PEEUdw9NFHs2LFCsC9i+CMM84AXBL5zne+w+TJkznkkEO47777gtvLysoKLj958mTOO+88hg4dykUXXRTsd2jevHkMHTqUSZMmcf311we361daWsqxxx7LuHHjGDduXLMEc9dddzFq1CjGjBnDrbfeCsCqVas44YQTGDNmDOPGjePLL79sFjPAtddey+zZswEoLi7m5z//OZMmTeKZZ57hoYce4sgjj2TMmDGce+65VFdXA+G7u77jjju49957g9u97bbbmh2Djkq8O4Ks3pCSYYnAJK5/3gqb9u13Z7/0GQWn/qrdq61cuZL58+eTnJzM7t27efPNN0lJSWH+/Pn85Cc/4bnnnttnneXLl/P6669TUVHB4Ycfzve//31SU1ObLfPRRx+xZMkS+vXrxzHHHMPbb79NSUkJV111FW+++SaDBw9usd+eXr168eqrr5KRkcEXX3zBjBkzWLx4Mf/85z954YUXeP/998nMzAz2PXTRRRdx6623Mm3aNGpqaoL9JbUmIyODt956C3DFZt/73vcA183En/70J6677rqw3V3369ePc845hxtuuIGmpibmzp3LwoUL233cQyVeIkhKgryB1pbAmDhw/vnnB4tGdu3axaWXXsoXX3yBiFBfXx92ndNPP5309HTS09Pp1asXmzdvpqioqNkyEyZMCE4bO3YspaWlZGVlccghhwS7fZ4xYwYPPvjgPtuvr6/n2muv5eOPPyY5OTnYoGv+/PlcfvnlZGZmAq5b6oqKCtavXx/srygjIyOi3+3vlvrzzz/n9ttvD3awd/LJJwPhu7vOzc2lZ8+efPTRR2zevJkjjjiCnj17RrTP1iReIgBrS2ASWweu3KPF3z3zHXfcwZQpU3j++ecpLS1l8uTJYdcJdDkNzbudbmuZSPtVu+eee+jduzeffPIJTU1NwZN7uG6iW9pme7qlvuyyy3jhhRcYM2YMs2fPZsGCBa3G993vfpfZs2ezadMmvvOd70T0m9qSeHUE4LUlsERgTDzxd88cKE/vTEOHDmX16tXBF9c89dRTLcbRt29fkpKSePzxx2lsdN3Wn3TSSTzyyCPBMvzt27eTk5NDUVERL7zwAuBeuFNdXc2gQYNYunQptbW17Nq1a583qvlVVFTQt29f6uvrg+9EgPDdXQNMmzaNl19+mUWLFgXvHvZXgiaCYqjdBXtaf6uQMebA+dGPfsSPf/xjjjnmmODJtzN169aN+++/n1NOOYVJkybRu3fvfbqlBtd99aOPPspRRx3FypUrg1fvp5xyCmeeeSYlJSWMHTs2+M7kxx9/nPvuu4/Ro0dz9NFHs2nTJgYMGBB8X/JFF13EEUcc0WJcd955JxMnTuTEE09s1n11uO6uAdLS0pgyZQrTp0/vtCeOEqsb6oClL8LTl8CVb0C/8O8xNaYrScRuqMOprKwkKysLVeWaa65hyJAhzV4OfzBoampi3LhxPPPMMwwZMiTsMu3thjpB7wh8bQmMMQnjoYceYuzYsYwYMYJdu3Zx1VVXxTqkdlm6dClf+9rXmDp1aotJoCMSs7I4v9h92iOkxiSUG2+88aC7A/AbPnw4q1ev7vTtJuYdQUYuZORZhbFJKAdbMbDpmI78nRMzEYArHrKiIZMgMjIy2LZtmyWDLk5V2bZtW8TtGQISs2gIXFuCLctiHYUxB0RRURFlZWWUl5fHOhQTZRkZGfs0sGtL4iaC/GJY+TI0NbnWxsZ0YampqcEWtcaEStwzYP4gaKxz7yYwxpgElriJIK/YfVqFsTEmwSVuIrC2BMYYAyRyIsgdAIjdERhjEl7iJoLUDMjua43KjDEJL3ETAVhbAmOMIdETgb2XwBhjEjwR5BfD7vXQUBfrSIwxJmaimghE5BQRWSEiq0Tk1jDzc0Xk7yLyiYgsEZHLoxnPPvIHAQq7Wn+/qDHGdGVRSwQikgz8ATgVGA7MEJHhIYtdAyxV1THAZOBuEUmLVkz7yPMeIbUKY2NMAovmHcEEYJWqrlbVOmAucFbIMgpki3sRaBawHdj3BaTRYm0JjDEmqomgP+Avcynzpvn9HhgGbAA+A25Q1aaQZRCRK0VksYgs7tROs7L7QnKaVRgbYxJaNBOBhJkW2gfuycDHQD9gLPB7EcnZZyXVB1W1RFVLCgsLOy/CpGTXsMyKhowxCSyaiaAMGOAbL8Jd+ftdDvxVnVXAV8BQDiRrS2CMSXDRTASLgCEiMtirAL4QeDFkmbXAVAAR6Q0cDnT+e9haY20JjDEJLmrvI1DVBhG5FngFSAYeUdUlInK1N38WcCcwW0Q+wxUl3aKqW6MVU1j5xbBnO9Tshox9SqWMMabLi+qLaVR1HjAvZNos3/cNwEnRjKFN/ieH+oyKaSjGGBMLid2yGHxtCax4yBiTmCwR5Be7T6swNsYkKEsE3fIhPcfuCIwxCcsSgYj35FBprCMxxpiYsEQA1pbAGJPQLBGAuyPYuRY0tOGzMcZ0fZYIwFUY11dDVSf2Y2SMMQcJSwSwty2B1RMYYxKQJQKwtgTGmIRmiQAgb6D73Fka0zCMMSYWLBEApGVCVm+7IzDGJCRLBAHWlsAYk6AsEQRYWwJjTIKyRBCQNwh2rYfGA/fKZGOMiQeWCALyi0EbYXdZrCMxxpgDyhJBgLUlMMYkKEsEAdaWwBiToCwRBOT0B0m2CmNjTMKxRBCQnAJ5A+yOwBiTcCwR+FlbAmNMArJE4GdtCYwxCcgSgV/eINcVdV1VrCMxxpgDxhKBX/BF9mtjGoYxxhxIlgj8AonA6gmMMQnEEoGftSUwxiQgSwR+3QsgtbtVGBtjEoolAj8R9+SQ3REYYxKIJYJQ1pbAGJNgLBGECrQlUI11JMYYc0BYIgiVXwx1lVC9PdaRGGPMAWGJIFTgySF7kb0xJkFYIghl7yUwxiSYNhOBiJwhIh1KGCJyioisEJFVInJrC8tMFpGPRWSJiLzRkf10KmtLYIxJMJGc4C8EvhCRu0RkWKQbFpFk4A/AqcBwYIaIDA9ZJg+4HzhTVUcA50ccebSkZ0FmgbUlMMYkjDYTgapeDBwBfAn8WUTeFZErRSS7jVUnAKtUdbWq1gFzgbNClvkW8FdVXevta0u7f0E0WFsCY0wCiajIR1V3A8/hTuZ9gWnAhyJyXSur9QfW+cbLvGl+hwH5IrJARD4QkW+H25CXeBaLyOLy8vJIQt4/1pbAGJNAIqkj+KaIPA+8BqQCE1T1VGAMcHNrq4aZFvpwfgowHjgdOBm4Q0QO22cl1QdVtURVSwoLC9sKef/lD4JdZdDUGP19GWNMjKVEsMz5wD2q+qZ/oqpWi8h3WlmvDBjgGy8CNoRZZquqVgFVIvImLsGsjCCu6MkvhqZ62L3Bvb7SGGO6sEiKhv4bWBgYEZFuIlIMoKr/bmW9RcAQERksImm4SucXQ5b5G3CsiKSISCYwEVgWefhREmxLYPUExpiuL5JE8AzQ5Btv9Ka1SlUbgGuBV3An96dVdYmIXC0iV3vLLANeBj7FJZuHVfXz9v2EKLC2BMaYBBJJ0VCK99QPAKpa513ht0lV5wHzQqbNChn/DfCbSLZ3wOQOAEmyJ4eMMQkhkjuCchE5MzAiImcBW6MXUhxIToWcIisaMsYkhEjuCK4GnhCR3+OeBFoHhH3Ms0uxtgTGmATRZiJQ1S+Bo0QkCxBVrYh+WHEgbxCsmh/rKIwxJuoiuSNARE4HRgAZIq55gKr+PIpxxV7+IKjcBPV7ILVbrKMxxpioiaRB2SzgAuA6XNHQ+cCgKMcVe/nF7nPnulYXM8aYg10klcVHq+q3gR2q+jPg6zRvKNY1WVsCY0yCiCQR1Hif1SLSD6gHBkcvpDhhbQmMMQkikjqCv3vdRf8G+BDXX9BDUY0qHmT1hpQMSwTGmC6v1UTgvZDm36q6E3hORF4CMlR11wGJLpZEXPGQFQ0ZY7q4VouGVLUJuNs3XpsQSSDA2hIYYxJAJHUE/xKRcyXw3GgisTsCY0wCiKSO4CagO9AgIjW4R0hVVXOiGlk8yB8ENbtgzw7olh/raIwxJioiaVnc1ispu65AW4IdaywRGGO6rDYTgYh8I9z00BfVdEn+tgT9xsY2FmOMiZJIiob+y/c9A/dS+g+A46MSUTwJtiWwegJjTNcVSdHQN/3jIjIAuCtqEcWTjFzIyLO2BMaYLi2Sp4ZClQEjOzuQuJVfbE8OGWO6tEjqCP4P15oYXOIYC3wSzaDiSv4g2Lw01lEYY0zURFJHsNj3vQGYo6pvRyme+JM3CFa8DE1NkNSRGyhjjIlvkSSCZ4EaVW0EEJFkEclU1erohhYn8gdBY617N0FOv1hHY4wxnS6SS9x/A/43s3QDEufVXf62BMYY0wVFkggyVLUyMOJ9z4xeSHEmr9h9WoWxMaaLiiQRVInIuMCIiIwH9kQvpDiTNwAQuyMwxnRZkdQR/BB4RkQ2eON9ca+uTAwp6ZDd19oSGGO6rEgalC0SkaHA4bgO55aran3UI4sn1pbAGNOFRfLy+muA7qr6uap+BmSJyA+iH1ocsfcSGGO6sEjqCL7nvaEMAFXdAXwveiHFobxBsHs9NNTFOhJjjOl0kSSCJP9LaUQkGUiLXkhxKH8QoLBrXawjMcaYThdJIngFeFpEporI8cAc4J/RDSvOBNsSlMYyCmOMiYpInhq6BbgS+D6usvgj3JNDicP/XgJjjOli2rwj8F5g/x6wGigBpgLLohxXfMnuC8lpVmFsjOmSWrwjEJHDgAuBGcA24CkAVZ1yYEKLI0lJkDfQioaMMV1Sa3cEy3FX/99U1Umq+n9AY3s2LiKniMgKEVklIre2styRItIoIue1Z/sHVN4gKxoyxnRJrSWCc4FNwOsi8pCITMXVEUTEe7roD8CpwHBghogMb2G5X+MqpeOXtSUwxnRRLSYCVX1eVS8AhgILgBuB3iLygIicFMG2JwCrVHW1qtYBc4Gzwix3HfAcsKW9wR9QeYNgz3ao2R3rSIwxplNFUllcpapPqOoZQBHwMdBiMY9Pf8D/4H2ZNy1IRPoD04BZrW1IRK4UkcUisri8vDyCXUdBwRD3ufCPsdm/McZESbteuaWq21X1j6p6fASLhytG0pDx3wG3BF5608p+H1TVElUtKSwsjDTczjXkZBh1Prz2C5g/EzT0pxhjzMEpknYEHVUGDDXzGlYAABlBSURBVPCNFwEbQpYpAeZ6DZcLgNNEpEFVX4hiXB2TnALTHoT0HHjrHtizE06/G5KSYx2ZMcbsl2gmgkXAEBEZDKzHPYr6Lf8Cqjo48F1EZgMvxWUSCEhKcif/jFx463+hZhdM+yOkJFaPG8aYriVqiUBVG0TkWtzTQMnAI6q6RESu9ua3Wi8Qt0TghP+Gbnnw6k+htgKmPwZpifPSNmNM1yJ6kJV1l5SU6OLFi2MdhvPBbPj7D2Hg1+Fbc92dgjHGxCER+UBVS8LNa1dlsQkx/jI47xEoWwSzz4DKGD3RZIwx+8ESwf4aeQ7MmAtbv4A/nwI7ratqY8zBxRJBZxhyAlzyvLsjeOQU2Loq1hEZY0zELBF0lkFfh8tegoYaeORk2PhJrCMyxpiIWCLoTH1Hw3degZQMV2ew5t1YR2SMMW2yRNDZCr4GV7wCWb3h8WnwxauxjsgYY1pliSAacovg8n+6/onmXAifPxfriIwxpkWWCKIlq9DVGRRNgGevgMV/jnVExhgTliWCaMrIhYufgyEnwks/dH0UHWQN+IwxXZ8lgmhLy4QLnoCR57peS5+cbi+4McbEFUsEB0JKGpzzEJz8/6D0bbj/KHj7Pmisj3VkxhhjieCASUqGr/8Arl0Ih0yGV++AB6dA2QexjswYk+AsERxouUVw4ZNwwV+gehs8PBXm/Ze9AtMYEzOWCGJBBIZ9E655HyZcCQsfgj9MgKUvWmWyMeaAs0QQSxk5cNpd8N1/Q/cCePoSmDPDOq4zxhxQlgjiQdF4+N4COOkX8NUb8IeJ8O4foLEh1pEZYxKAJYJ4kZwCR18HP3gPio+BV34CDx8PGz6KdWTGmC7OEkG8yR8E33oazp8NFZvgoePhn7e6V2IaY0wUWCKIRyIwYhpcuwhKvgPvz3LFRcv+Dk1NsY7OGNPFWCKIZxm5cPrdcMW/ICMPnroYfjcSXv4xrH3fkoIxplPYy+sPFo31sOR5N6yaD411kNMfhp3p7h6KjoQky+vGmPBae3m9JYKDUc0uWPEyLH1hb1LI7gfDz4IRZ7seTy0pGGN8LBF0ZTW7YeXLsCSQFGq9pHAmDD8bBky0pGCMsUSQMGp2w8pXfMVHtZDdd2/xkSUFYxKWJYJEFEgKS19wr8tsrIWsPq7Du8HfgMHHQt7AWEdpjDlAWksEKQc6GHOAZOTA6PPdUFvhksLyl2DVq/DpXLdMfjEUHwuDj3OJIbtPTEM2xsSG3REkmqYmKF8GX70JX/0H1rzlKp8BCg7zEsM33Gf3nrGN1RjTaaxoyLSsqRE2feqSwldvwpp3oL7Kzes90ksMx8KgY6BbXmxjNcZ0mCUCE7nGete/0VdvuOSw7n1oqAFJgt4jIKcIMntCZg9v6OmGbv7vee5FPMaYuGF1BCZyyakwYIIbvvFfUF8D6xe7u4V1C2FXGWz8xL1Up7G2hY2ISwahCSKzh6uHyO7jnmbK7uu+p3Y7oD/RGNOcJQLTutQMKJ7kBj9VqK92CaF6u/vcs8Mb902r3ga7y1zxU9XW8MkjI29vUsjp50sUvoSR1cslqc7Q1OSKv2oroa7SVabXVfrGd/u+V0JdhftsqHH1KEVHQlGJi9WYLsASgekYEUjr7oZIH0NVhZqdrlfVio3uc/eG5uOrF7hPbQzdobur2J9koE1QV+1O8ERSJCqQlgXpWZCeDUmpsOrf0HSfm53T3yWE/iUuOfQba3c35qAU1UQgIqcA9wLJwMOq+quQ+RcBt3ijlcD3VfWTaMZkYkgEuuW7odewlpdraoLqrS457N64N0lUbg6TINoVgJe8vJN7mneCT8/2TcveOy81c98GePU1sOkzV1xWtgjKFsPSv7l5SSmuHqXoSDf0L4Geh7rfbUwci1plsYgkAyuBE4EyYBEwQ1WX+pY5GlimqjtE5FRgpqpObG27Vlls4k7lFpcQAslh/YfeXQcu6fUv2XvnkDcAuhe66ZYgzAEUq8riCcAqVV3tBTEXOAsIJgJVfce3/HtAURTjMSY6snrB0NPcAO6R3PIV3h3DIlj/ASyYT7PiqKRUlxCyCqF7L7eN7oXeZ6+907sXuiIx6xrERFE0E0F/wP8W9jKgtav9K4B/hpshIlcCVwIMHGjdIpg4l5QMvYe7YfylblrNbldhvnsjVG1xdxFV5d7nFtiy1H1vqt93e5IEmQWQ1dtVUOf2h9wi9yhv4Ht2P0hJO7C/03QZ0UwE4e57w5ZDicgUXCKYFG6+qj4IPAiuaKizAjTmgMnI2ffJq1CByvTK8vDJonIL7F7v7jL2bA9ZWVyiCJckcorcZ/dCu7MwYUUzEZQBA3zjRcCG0IVEZDTwMHCqqm6LYjzGxDd/ZXrhYa0vW1flnrjatQ52rXcJIvB981LX0WB9dfN1klJdRXhqpnu6KbUbpHifwWmBz4ww0zJdQulxiEsqVsfRZUQzESwChojIYGA9cCHwLf8CIjIQ+CtwiaqujGIsxnQtad2hYIgbwlF17Tp2lXlJwvusrYSGPVAfGKrd554d+05r2NPK/rOgx2CXFEKHrD6xu/NoanTtVSo27PuYcuUWlwhD26gEvqdlxibmOBC1RKCqDSJyLfAK7vHRR1R1iYhc7c2fBfwU6AncL+7qoqGlWm1jTDuI7O0GpO/ojm1D1TWiCySHuirYuQ62r947bF4Ky+c1r9tI6eZLEr5kkT/Y3VV0lDa5Bor7nOTbesRYXCV8Vi/YVgW754VPcum5kNM3fIPGwHhW7y5ZF2N9DRlj9k9To7vj8CeI7V+5zx1fuWQSLd3y9z1ZB07gOd607r0g2XfNq+paj/sTSMVG37Bp77Smhn33mVmwd9vNkoavVXz3grjrb8v6GjLGRE9SMuQPcsOhU5rPa2pyV/DbV8OOUmhoqX+qCGX23HuSz+rj6jLaSwQyct3Qa2jLyzUF7kACyWEDVGxufkey8RNX5BT6HIwke095hSSojLyWGzAGPmOQQCwRGGOiJynJVTDnFrn3XBxMkpJce46swtaL1xobXJFUszuLTXuTx/bVsOZtVw8TidTuIcnBlyyGngYjz+2c3+djicAYY/ZHcor3qG7/1perr/E6NKwI09FhmHH/tEBFf5+RUfkJlgiMMeZASM1wQ1avWEeyD2tdYowxCc4SgTHGJDhLBMYYk+AsERhjTIKzRGCMMQnOEoExxiQ4SwTGGJPgLBEYY0yCswZlxhizH+obm9hWWcfWylrKK2vZWlHLVm88OFTUsa2qDhHonpZMt7QU7zOZ7mkpZKYlk5meTGbge5r/u/vsnp5Mv7xu9M3t1um/wRKBMabDVJWa+iaq6xqormukuq6RqroG9tQ1UlXbwJ76RqpqG6n2pjXtZ2fH6alJ+55I01Poluo+/SfR5KS2X5yjqtQ2NLm4g/F68dc1Nvtd1bUNbKvyn+Dd953VYV4vCnRLTaYgO42CrHQG9sxk3KA8VKGqrpE93nZ31zSweXcNVbWNwX3XNjS1GO/Vxx3Krae20lFeB1kiMMYAUFXb0OwEF7iSDXzfVllHRW0De+oa3EmytoHq+kbitSf79JSkkCvrZOoaNRj/Hu9E357klJWeQkGWO7l/rTCLow7pQUFWenAo9E78BVnpdE/v2Om1sUmDiTM0GfXP60BvqxGwRGBMF9LQ2BQ8yfmvzKu9q83tVXVsrailPEzRxZ760Be6OHmZqRRkpdOzexr987rRPT180UXwe3oymd4VeqDoo5s3P3k/Xm+pQG2Du8MI/L7qZidK77O2sdl44DhU1zWSmpwULGbplpriPn0xdg/5Ld28ZTNT3fe0lOhXqyYnCdkZqWRnpEZ9XwGWCEzCCRQHNDtZBk4ctY1U13tXuyFXYzX1jaSlJLVQjus7CYacWDJSkxDfCTCS/e+pa2hWXBAaS2iRhYu5kbrGlosVAkSgR6Z35ZqdxriB+b6r2jQKstMp9MZ7dE87ICe/SLnjbaetzmZH1HQqVaW8spa126pZu72aNduqWbe9mjXbq9m0q4bsjBQKs30nncAJKNuNF3onn5TkyE8+jU3qrnRDrnCDlXeV7ip4e1Vd8Oq4sR3lASlJQmZaMhmpydQ1NkV8wg0QgcxUb/2Gpg7v359oMtNS6NE9jQH5gYSzb7m5P1EFklJ+91R6ZLbv+JquzxKBabfahkbKduxh7fbqfU74a7dXNytiEIG+ORkM6JHJkcX5VNY2UF5Zx+ryKrZW1oatGBOB/My05okiK528zFR276nfpxx7e1Vd2HLetOSk4BVun9wMRvTLISsjxXe1vu/JNdwTG+GuiOsbXQXjnpCr83AVpYGKwer6RtJTklrcf6CoIpL9G9OZLBGYFlXXNbBycyXLN+5m+aYKVmyqYM22KjburmlWQdgtNZmBPTIZ0COTSUMKGNgj0w09M+mf142M1PCv3lNVKmsb9p7UK2q9q/i949uq6vikbCdbK2qpqmskIzUpmBiK8jM5YmBes2QROPEXZKWTk5HSrEimM6UmJ5HbLYncbgeuHNeYaLFEEKGq2gYWr9lBapKQmb7vlWN6SlLUTjrR1tSkrNtRzbKNFSzftJvl3uea7dXBE373tGQO65PNUYf0ZGBPd6If1NOd/Auz0jv020X2VooNLuje5vJ1DU2kJstBe5yNiVeWCNqwdMNunly4hhc+2kBlbUOLyyUJ4YsW0lPITHWNRXIyUjm0sDtD++ZweJ9scg7gUwEBu/bUs2KTO9EHTvwrNlVQXeeKc0RgcM/uDO+XwznjihjaJ5thfXPon9eNpAiey44mKyIxJjosEYSxp66Rlz7dwJML1/LR2p2kpSRxxqi+nH1Ef9JTkoJlwaFPbIR7lG3Xnno27dpDVa377k8m/fO6MaxvNkP75DDU+yzumbnfFXm7qutZs71qn7L70q1VbNhVE1wuLzOVoX2ymV4yIBjHYb2z6ZYWvijHGNM1WSLw+WJzBU+8v5a/fljG7poGDinszu2nD+PccUXkd0/b7+2rKht21bAieDVewfKNu3l9RXnwKZL0lCQO653N0D7ZDO2bwzDvs4dv/w2NTWzcVeMqa5s9mVPF2m3V7K5pfudSkJXOwB7dmHhIT7ftvtkM65ND75yOFekYY7oW0XhtFtiCkpISXbx4cadtr6a+kZc/38ST769lYel2UpOFU0b25VsTBnLUIT0OyImypr6RVVsqg4lh+aYKlm3czbaquuAyvbLTGdQzk/KKWsp27KHB95hMarJQlJ8ZrKQNlN0P6pnJgPzMDrdwNMZ0HSLygaqWhJuXsGeI1eWVzFm4lmc/KGNHdT2DemZy66lDOW98EQVZ6Qc0lozUZEb2z2Vk/9xm08sraoOVt8s27Wbd9mpG9M/ltFF9g0/lDOyRSd/cbhH1q2KMMeEkVCKoa2jiX0vd1f87X24jOUk4aXhvvjVxIMccWhDzytBQhdnpFGYXcuyQwliHYozpwhImEby2fDM/evZTtlbW0T+vGzefdBjTSwbQKyc6nTgZY8zBImESwcAemYwdkMdFEwfxjcMKrSjFGGM8CZMIvtYrm4cvPTLWYRhjTNyxFjrGGJPgLBEYY0yCi2oiEJFTRGSFiKwSkVvDzBcRuc+b/6mIjItmPMYYY/YVtUQgIsnAH4BTgeHADBEZHrLYqcAQb7gSeCBa8RhjjAkvmncEE4BVqrpaVeuAucBZIcucBTymzntAnoj0jWJMxhhjQkQzEfQH1vnGy7xp7V0GEblSRBaLyOLy8vJOD9QYYxJZNBNBuAf1Qzs2imQZVPVBVS1R1ZLCQmtla4wxnSmaiaAMGOAbLwI2dGAZY4wxURS13kdFJAVYCUwF1gOLgG+p6hLfMqcD1wKnAROB+1R1QhvbLQfWtDC7ANi6/9FHTbzHB/Efo8W3fyy+/XMwxzdIVcMWqUStZbGqNojItcArQDLwiKouEZGrvfmzgHm4JLAKqAYuj2C7LZYNicjilrpZjQfxHh/Ef4wW3/6x+PZPV40vql1MqOo83MneP22W77sC10QzBmOMMa2zlsXGGJPguloieDDWAbQh3uOD+I/R4ts/Ft/+6ZLxHXSvqjTGGNO5utodgTHGmHayRGCMMQmuyySCtno6jTURKRWRz0TkYxFZHAfxPCIiW0Tkc9+0HiLyqoh84X3mx1l8M0VkvXcMPxaR02IY3wAReV1ElonIEhG5wZseF8ewlfji4hiKSIaILBSRT7z4fuZNj5fj11J8cXH8fHEmi8hHIvKSN96h49cl6gi8nk5XAifiWisvAmao6tKYBuYjIqVAiarGRWMUEfkGUInr9G+kN+0uYLuq/spLpvmqekscxTcTqFTV38YiJj+vc8S+qvqhiGQDHwBnA5cRB8ewlfimEwfHUEQE6K6qlSKSCrwF3ACcQ3wcv5biO4U4OH4BInITUALkqOoZHf0/3FXuCCLp6dT4qOqbwPaQyWcBj3rfH8WdOGKihfjihqpuVNUPve8VwDJch4lxcQxbiS8ueD0OV3qjqd6gxM/xaym+uCEiRcDpwMO+yR06fl0lEUTUi2mMKfAvEflARK6MdTAt6K2qG8GdSIBeMY4nnGu9lxg9EsuiKz8RKQaOAN4nDo9hSHwQJ8fQK9b4GNgCvKqqcXX8WogP4uT4Ab8DfgQ0+aZ16Ph1lUQQUS+mMXaMqo7DvYznGq/ow7TPA8ChwFhgI3B3bMMBEckCngN+qKq7Yx1PqDDxxc0xVNVGVR2L62xygoiMjFUs4bQQX1wcPxE5A9iiqh90xva6SiKI+15MVXWD97kFeB5XnBVvNntly4Ey5i0xjqcZVd3s/edsAh4ixsfQKzt+DnhCVf/qTY6bYxguvng7hl5MO4EFuPL3uDl+Af744uj4HQOc6dU9zgWOF5G/0MHj11USwSJgiIgMFpE04ELgxRjHFCQi3b0KO0SkO3AS8Hnra8XEi8Cl3vdLgb/FMJZ9SPO3100jhsfQq0z8E7BMVf/XNysujmFL8cXLMRSRQhHJ8753A04AlhM/xy9sfPFy/FT1x6papKrFuPPda6p6MR09fqraJQZcL6YrgS+B22IdT0hshwCfeMOSeIgPmIO7ta3H3VFdAfQE/g184X32iLP4Hgc+Az71/sH3jWF8k3DFj58CH3vDafFyDFuJLy6OITAa+MiL43Pgp970eDl+LcUXF8cvJNbJwEv7c/y6xOOjxhhjOq6rFA0ZY4zpIEsExhiT4CwRGGNMgrNEYIwxCc4SgTHGJDhLBCamRERF5G7f+M1e53Kdse3ZInJeZ2yrjf2c7/Xy+XonbOvnInJCG8vMFJGbw0wvFl9vrcZEyhKBibVa4BwRKYh1IH5ej7aRugL4gapO2d/9qupPVXX+/m6nI9r5m00XYonAxFoD7j2rN4bOCL2iF5FK73OyiLwhIk+LyEoR+ZWIXOT1H/+ZiBzq28wJIvIfb7kzvPWTReQ3IrLI6zzsKt92XxeRJ3GNhkLjmeFt/3MR+bU37ae4xluzROQ3IctPFpEFIvKsiCwXkSe8Fr+IyHjvN3wgIq/4ugUI/mYROc1b7y0RuU+8Puc9w71trxaR633TU0TkUe93PSsimd62porrt/4zr7O0dG96qYj8VETeAs4XketFZKm3/twI/n6mK4h1qzgbEnvAvXMgBygFcoGbgZnevNnAef5lvc/JwE6gL5AOrAd+5s27Afidb/2XcRc8Q3AtlDOAK4HbvWXSgcXAYG+7VcDgMHH2A9YChUAK8BpwtjdvAe5dE6HrTAZ24fq+SgLexSWNVOAdoNBb7gLgEf9v9uJcF4gF19I60Hp0prd+OlAAbPO2WYxrTXyMt9wj3vEMbOswb/pjuE7o8I77j3wxbwDSve95sf73YcOBGeyOwMScul4xHwOub2tZn0Xq+tyvxXUr8i9v+me4E2LA06rapKpfAKuBobi+nr4trovh93HN8od4yy9U1a/C7O9IYIGqlqtqA/AEEEkPsgtVtUxdJ2Ufe7EdDowEXvViuB2XLPyGAqt9scwJmf8PVa1V96KjLUBvb/o6VX3b+/4XXOI5HPhKVVd60x8Nif0p3/dPgSdE5GLc3ZpJACmxDsAYz++AD4E/+6Y14BVfekUqab55tb7vTb7xJpr/uw7tQ0Vx3ZZfp6qv+GeIyGTcHUE44bo6j4Q/zkYvNgGWqOrXW1mvrf2F2y60/Htb4//Np+OSxJnAHSIywkt8pguzOwITF1R1O/A0ruI1oBQY730/C1f80V7ni0iSV29wCLACeAX4vtdNMyJymNcrbGveB44TkQKvUnUG8EYH4sGLoVBEvu7tP1VERoQssxw4RNxLZcAVH0ViYGC7XoxvedsqFpGvedMvCRe7iCQBA1T1ddwLT/KArAj3aw5idkdg4sndwLW+8YeAv4nIQlxPii1drbdmBe6k1xu4WlVrRORhXBHNh96dRjltvNJPVTeKyI+B13FX2PNUtUNdJKtqnVchfJ+I5OL+H/4O1zNtYJk9IvID4GUR2QosjHDzy4BLReSPuB4oH/B+8+XAMyKSguu2fVaYdZOBv3gxCXCPur74TRdnvY8aE6dEJEvdy9MF+APwhareE+u4TNdjRUPGxK/veZXJS3BPVP0xxvGYLsruCIwxJsHZHYExxiQ4SwTGGJPgLBEYY0yCs0RgjDEJzhKBMcYkuP8POYeiygobPuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate plot\n",
    "plt.title('k-NN Varying number of neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved a slightly better score with the KNN, it outperforms the Dummy Classifier, but not by much with only 12.28% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10877192982456141"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#which iteration is the best:\n",
    "(list(test_accuracy).index(max(test_accuracy))*2)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we notice the KNN does not perform as well as we wanted, we try all the available classification algorithms in the Scikit-Learn library. This method is very efficient in the way that it allows testing a lot of algorithms but is not perfect: we test each algorithm with the default configuration. Which means that to us, the algorithms with pretty good default parameters (for our dataset) will perform well compared to other methods with worse default parameters but that can be tuned to get a better accuracy. However one of the solutions against this would be to perform a grid search for each one of the classification algorithm, hence getting the real best algorithm in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() missing 1 required positional argument: 'base_estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='classifier')\n",
    "\n",
    "all_clfs = []\n",
    "for name, ClassifierClass in estimators:\n",
    "    #print('Appending', name)\n",
    "    try:\n",
    "        clf = ClassifierClass()\n",
    "        all_clfs.append(clf)\n",
    "    except Exception as e:\n",
    "        #print('Unable to import', name)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08771929824561403\n",
      "0.14736842105263157\n",
      "0.11929824561403508\n",
      "0.11578947368421053\n",
      "0.07368421052631578\n",
      "0.08070175438596491\n",
      "0.12982456140350876\n",
      "0.07368421052631578\n",
      "0.07368421052631578\n",
      "0.14035087719298245\n",
      "0.10175438596491228\n",
      "0.10175438596491228\n",
      "0.09473684210526316\n",
      "0.14385964912280702\n",
      "0.06315789473684211\n",
      "0.03508771929824561\n",
      "0.03859649122807018\n",
      "0.042105263157894736\n",
      "0.07017543859649122\n",
      "0.09473684210526316\n",
      "0.07017543859649122\n",
      "0.08070175438596491\n",
      "0.10526315789473684\n",
      "0.07719298245614035\n",
      "specified nu is infeasible\n",
      "0.06315789473684211\n",
      "0.07719298245614035\n",
      "0.0035087719298245615\n",
      "No neighbors found for test samples array([  1,   6,   7,   8,  10,  11,  14,  16,  18,  28,  31,  48,  49,\n",
      "        54,  58,  59,  60,  66,  74,  76,  77,  81,  82,  88,  90,  91,\n",
      "       106, 121, 124, 135, 146, 155, 157, 160, 161, 165, 170, 176, 177,\n",
      "       183, 197, 198, 203, 217, 222, 229, 235, 244, 262, 263, 264, 272,\n",
      "       283], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n",
      "0.1368421052631579\n",
      "0.09473684210526316\n",
      "0.0912280701754386\n",
      "0.05964912280701754\n",
      "0.08771929824561403\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for algo in all_clfs:\n",
    "    try:\n",
    "        algo.fit(X_train, y_train)\n",
    "        performance = algo.score(X_test, y_test)\n",
    "        results.append(performance)\n",
    "        print(performance)\n",
    "    except Exception as e:\n",
    "        #print('Unable to import', name)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the best algorithm of all of them, performing 17.54% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier()"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_clfs[results.index(max(results))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Regressor (as baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that the classification algorithms perform poorly on this dataset we chose to switch to a regression approach: we can predict a float between 1997 and 2019 then round the number to the closest integer, hence getting a classification if we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first perform the dummy regressor to get the baseline for our regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0032957216915714405"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "dummy_regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dummy_regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a MSE of 26 with the dummy regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.732491807017574"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'base_estimator'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='regressor')\n",
    "\n",
    "all_regs = []\n",
    "for name, ClassifierClass in estimators:\n",
    "    #print('Appending', name)\n",
    "    try:\n",
    "        clf = ClassifierClass()\n",
    "        all_regs.append(clf)\n",
    "    except Exception as e:\n",
    "        #print('Unable to import', name)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to stop the cell from running cause it was way too long, don't really know why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21472528538602476\n",
      "0.3925030802651095\n",
      "0.3025470310107716\n",
      "0.43743107871773657\n",
      "-0.2283519784138861\n",
      "-0.07934145015175642\n",
      "-0.001385750811327524\n",
      "-0.001385750811327524\n",
      "0.4446741141969849\n",
      "-0.25863633208251535\n",
      "0.4320230920045345\n",
      "-0.0013854894201472678\n",
      "-2.24809687465523\n",
      "0.41561971709032175\n",
      "0.42548531615460916\n",
      "0.13790167028582845\n",
      "Isotonic regression input X should be a 1d array or 2d array with 1 feature\n",
      "0.24005898641326606\n",
      "-3213.9010161027704\n",
      "-1.3821997058455286\n",
      "0.37110911505607946\n",
      "-0.001385750811327524\n",
      "0.44727130231680046\n",
      "-0.001385750811327524\n",
      "0.452817858134569\n",
      "0.37804332912157046\n",
      "-2.237326752920066\n",
      "-27035.200341161075\n",
      "-33981.43932293607\n",
      "For mono-task outputs, use ElasticNet\n",
      "For mono-task outputs, use ElasticNetCVCV\n",
      "For mono-task outputs, use ElasticNet\n",
      "For mono-task outputs, use LassoCVCV\n",
      "0.2588807176891601\n",
      "0.28713971326354426\n",
      "0.3132274814609456\n",
      "-9.573772574889984\n",
      "0.44283119674092064\n",
      "0.1396656482805827\n",
      "0.43581026692117786\n",
      "-56.990047751394066\n",
      "Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "0.4006852868043209\n",
      "0.4387807231400016\n",
      "0.4387807231399983\n",
      "-6.927044545580924\n",
      "0.2903246456716636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-0e8085bd73bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_regs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mperformance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperformance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_theil_sen.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    385\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_lstsq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             for job in range(n_jobs))\n\u001b[0m\u001b[0;32m    388\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         self.n_iter_, coefs = _spatial_median(weights,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_theil_sen.py\u001b[0m in \u001b[0;36m_lstsq\u001b[1;34m(X, y, indices, fit_intercept)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0my_subpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_subsamples\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         weights[index] = lstsq(X_subpopulation,\n\u001b[1;32m--> 191\u001b[1;33m                                y_subpopulation)[1][:n_features]\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_regs = []\n",
    "for algo in all_regs:\n",
    "    try:\n",
    "        algo.fit(X_train, y_train)\n",
    "        performance = algo.score(X_test, y_test)\n",
    "        results.append(performance)\n",
    "        print(performance)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try the two best regression method using the same approach to try all the regressors fro Scikit-Learn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44202319914570065"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor(random_state=1)\n",
    "reg.fit(X_train, y_train)\n",
    "GradientBoostingRegressor(random_state=0)\n",
    "y_pred = reg.predict(X_test)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a MSE of 15.01, better than the Dummy Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] > 2019:\n",
    "        y_pred[i] = 2019\n",
    "    if y_pred[i] < 1997:\n",
    "        y_pred[i] = 1997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.863130406331823"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(random_state=1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.025, 0.05, 0.075, 0.1],\n",
       "                         'max_depth': [4, 5, 6], 'max_features': [1.0],\n",
       "                         'min_samples_leaf': [3],\n",
       "                         'n_estimators': [100, 200, 300, 400, 500]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid={'n_estimators':[100,200,300,400,500], \n",
    "            'learning_rate': [0.025,0.05,0.075,0.1],\n",
    "            'max_depth':[4,5,6], \n",
    "            'min_samples_leaf':[3], \n",
    "            'max_features':[1.0] } \n",
    "\n",
    "reg = GradientBoostingRegressor(random_state=1)\n",
    "grid_search = GridSearchCV(estimator = reg, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714774554586725"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a MSE of 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] > 2019:\n",
    "        y_pred[i] = 2019\n",
    "    if y_pred[i] < 1997:\n",
    "        y_pred[i] = 1997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.074983510893242"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42548531615460916"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "est = HistGradientBoostingRegressor().fit(X_train, y_train)\n",
    "est.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a MSE of 14.6 too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.672323444954957"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = est.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-198.36318593195455"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor(hidden_layer_sizes=500, random_state=1, max_iter=200000).fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tfidf_matrix.toarray()\n",
    "a = a.tolist()\n",
    "\n",
    "X_train = a\n",
    "y_train = list(df_fr['date'])\n",
    "\n",
    "a = []\n",
    "for item in y_train:\n",
    "    a.append(float(item))\n",
    "    \n",
    "y_train = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = numpy.array(X_train)\n",
    "y_train = numpy.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 849)               721650    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2670)              2269500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2670)              7131570   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 2671      \n",
      "=================================================================\n",
      "Total params: 10,125,391\n",
      "Trainable params: 10,125,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(2670, activation='relu'))\n",
    "model.add(Dense(2670, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "13/13 [==============================] - 1s 80ms/step - loss: 3385669.7500 - mse: 3385669.5000 - mae: 1823.7772 - val_loss: 866231.6875 - val_mse: 866231.6875 - val_mae: 915.4318\n",
      "Epoch 2/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 445766.6875 - mse: 445766.6875 - mae: 554.6404 - val_loss: 432145.0000 - val_mse: 432145.0000 - val_mae: 625.9157\n",
      "Epoch 3/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 246234.2031 - mse: 246234.2031 - mae: 419.9730 - val_loss: 139544.4531 - val_mse: 139544.4531 - val_mae: 305.5433\n",
      "Epoch 4/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 120356.7422 - mse: 120356.7422 - mae: 283.4302 - val_loss: 72900.4375 - val_mse: 72900.4375 - val_mae: 219.0452\n",
      "Epoch 5/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 86224.7500 - mse: 86224.7500 - mae: 235.3320 - val_loss: 80417.0000 - val_mse: 80417.0000 - val_mae: 225.7081\n",
      "Epoch 6/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 64874.0117 - mse: 64874.0117 - mae: 203.1620 - val_loss: 65022.5156 - val_mse: 65022.5156 - val_mae: 201.4223\n",
      "Epoch 7/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 52208.2578 - mse: 52208.2578 - mae: 182.6239 - val_loss: 57522.7383 - val_mse: 57522.7344 - val_mae: 192.5249\n",
      "Epoch 8/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 47132.6992 - mse: 47132.6992 - mae: 173.4147 - val_loss: 53966.1328 - val_mse: 53966.1289 - val_mae: 185.8969\n",
      "Epoch 9/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 40504.0938 - mse: 40504.0938 - mae: 160.7621 - val_loss: 51209.1641 - val_mse: 51209.1641 - val_mae: 180.1090\n",
      "Epoch 10/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 36331.3398 - mse: 36331.3398 - mae: 151.6323 - val_loss: 49611.0234 - val_mse: 49611.0234 - val_mae: 177.1366\n",
      "Epoch 11/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 32595.4258 - mse: 32595.4258 - mae: 145.1184 - val_loss: 47781.0469 - val_mse: 47781.0469 - val_mae: 173.6780\n",
      "Epoch 12/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 29933.5703 - mse: 29933.5703 - mae: 138.0500 - val_loss: 47073.0508 - val_mse: 47073.0508 - val_mae: 171.8028\n",
      "Epoch 13/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 26909.1211 - mse: 26909.1211 - mae: 131.1245 - val_loss: 44116.5234 - val_mse: 44116.5234 - val_mae: 167.2910\n",
      "Epoch 14/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 22680.4961 - mse: 22680.4961 - mae: 120.4789 - val_loss: 42204.9023 - val_mse: 42204.9023 - val_mae: 163.9285\n",
      "Epoch 15/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 19846.7812 - mse: 19846.7812 - mae: 112.4209 - val_loss: 40739.0938 - val_mse: 40739.0938 - val_mae: 160.4158\n",
      "Epoch 16/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 17728.4082 - mse: 17728.4082 - mae: 106.0989 - val_loss: 39515.0469 - val_mse: 39515.0469 - val_mae: 157.5097\n",
      "Epoch 17/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 16329.2275 - mse: 16329.2275 - mae: 101.9528 - val_loss: 40312.7227 - val_mse: 40312.7227 - val_mae: 157.9368\n",
      "Epoch 18/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 14464.0908 - mse: 14464.0908 - mae: 96.5261 - val_loss: 36578.9219 - val_mse: 36578.9219 - val_mae: 151.0294\n",
      "Epoch 19/150\n",
      "13/13 [==============================] - 1s 66ms/step - loss: 12649.5078 - mse: 12649.5059 - mae: 90.4938 - val_loss: 35290.4062 - val_mse: 35290.4062 - val_mae: 147.9014\n",
      "Epoch 20/150\n",
      "13/13 [==============================] - 1s 67ms/step - loss: 11664.4922 - mse: 11664.4922 - mae: 86.7451 - val_loss: 33485.7227 - val_mse: 33485.7227 - val_mae: 144.9156\n",
      "Epoch 21/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 10493.6846 - mse: 10493.6846 - mae: 81.6183 - val_loss: 32255.7246 - val_mse: 32255.7246 - val_mae: 142.1315\n",
      "Epoch 22/150\n",
      "13/13 [==============================] - 1s 67ms/step - loss: 8844.6768 - mse: 8844.6768 - mae: 75.6082 - val_loss: 31442.9258 - val_mse: 31442.9258 - val_mae: 139.8712\n",
      "Epoch 23/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 7862.1831 - mse: 7862.1831 - mae: 71.2459 - val_loss: 30603.5723 - val_mse: 30603.5723 - val_mae: 138.2752\n",
      "Epoch 24/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 7209.4297 - mse: 7209.4297 - mae: 67.8863 - val_loss: 29910.2129 - val_mse: 29910.2129 - val_mae: 135.0789\n",
      "Epoch 25/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 6274.8496 - mse: 6274.8496 - mae: 63.5781 - val_loss: 28984.2285 - val_mse: 28984.2305 - val_mae: 132.6570\n",
      "Epoch 26/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 5532.0015 - mse: 5532.0015 - mae: 59.5215 - val_loss: 28060.9785 - val_mse: 28060.9785 - val_mae: 131.0536\n",
      "Epoch 27/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 5078.2798 - mse: 5078.2798 - mae: 57.1231 - val_loss: 29001.3711 - val_mse: 29001.3711 - val_mae: 132.6140\n",
      "Epoch 28/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 4881.2812 - mse: 4881.2812 - mae: 55.2517 - val_loss: 26876.2227 - val_mse: 26876.2227 - val_mae: 127.3282\n",
      "Epoch 29/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 4085.3137 - mse: 4085.3137 - mae: 50.3968 - val_loss: 25746.0957 - val_mse: 25746.0957 - val_mae: 125.0020\n",
      "Epoch 30/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 3521.2922 - mse: 3521.2922 - mae: 46.8519 - val_loss: 25170.8945 - val_mse: 25170.8945 - val_mae: 123.4616\n",
      "Epoch 31/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 3170.0107 - mse: 3170.0107 - mae: 44.3150 - val_loss: 24882.0586 - val_mse: 24882.0586 - val_mae: 122.3356\n",
      "Epoch 32/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 2819.2112 - mse: 2819.2112 - mae: 41.9017 - val_loss: 23949.0742 - val_mse: 23949.0742 - val_mae: 120.4962\n",
      "Epoch 33/150\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 2488.3613 - mse: 2488.3613 - mae: 39.1510 - val_loss: 23298.5605 - val_mse: 23298.5605 - val_mae: 119.0256\n",
      "Epoch 34/150\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 2235.2754 - mse: 2235.2754 - mae: 37.1828 - val_loss: 22926.3125 - val_mse: 22926.3125 - val_mae: 117.6287\n",
      "Epoch 35/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 1967.9033 - mse: 1967.9033 - mae: 34.5895 - val_loss: 22311.1289 - val_mse: 22311.1289 - val_mae: 116.3389\n",
      "Epoch 36/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 1759.7531 - mse: 1759.7531 - mae: 32.8825 - val_loss: 22205.1348 - val_mse: 22205.1348 - val_mae: 115.8269\n",
      "Epoch 37/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 1564.7451 - mse: 1564.7451 - mae: 30.7569 - val_loss: 21344.8203 - val_mse: 21344.8203 - val_mae: 113.9317\n",
      "Epoch 38/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 1455.4939 - mse: 1455.4939 - mae: 29.9773 - val_loss: 21015.0488 - val_mse: 21015.0488 - val_mae: 112.6448\n",
      "Epoch 39/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 1263.7267 - mse: 1263.7267 - mae: 27.7420 - val_loss: 20636.7500 - val_mse: 20636.7500 - val_mae: 111.6420\n",
      "Epoch 40/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 1197.9825 - mse: 1197.9825 - mae: 27.1423 - val_loss: 20274.1680 - val_mse: 20274.1680 - val_mae: 110.9203\n",
      "Epoch 41/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 987.4161 - mse: 987.4161 - mae: 24.6986 - val_loss: 19922.7793 - val_mse: 19922.7793 - val_mae: 109.8639\n",
      "Epoch 42/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 847.7144 - mse: 847.7144 - mae: 22.6922 - val_loss: 19483.1934 - val_mse: 19483.1934 - val_mae: 108.7525\n",
      "Epoch 43/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 761.9058 - mse: 761.9058 - mae: 21.4652 - val_loss: 19118.5332 - val_mse: 19118.5332 - val_mae: 107.6630\n",
      "Epoch 44/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 664.2723 - mse: 664.2723 - mae: 20.0723 - val_loss: 19002.8359 - val_mse: 19002.8359 - val_mae: 107.2441\n",
      "Epoch 45/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 614.0669 - mse: 614.0669 - mae: 19.4870 - val_loss: 18441.8789 - val_mse: 18441.8789 - val_mae: 106.0579\n",
      "Epoch 46/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 527.3622 - mse: 527.3622 - mae: 17.8685 - val_loss: 18357.4141 - val_mse: 18357.4141 - val_mae: 105.5203\n",
      "Epoch 47/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 471.8281 - mse: 471.8281 - mae: 16.8973 - val_loss: 18035.8359 - val_mse: 18035.8359 - val_mae: 104.6191\n",
      "Epoch 48/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 431.6207 - mse: 431.6207 - mae: 16.1142 - val_loss: 17670.1465 - val_mse: 17670.1465 - val_mae: 103.7937\n",
      "Epoch 49/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 365.1699 - mse: 365.1699 - mae: 14.9319 - val_loss: 17411.8242 - val_mse: 17411.8242 - val_mae: 103.2505\n",
      "Epoch 50/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 328.3143 - mse: 328.3143 - mae: 14.1196 - val_loss: 17268.7461 - val_mse: 17268.7461 - val_mae: 102.5296\n",
      "Epoch 51/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 284.1432 - mse: 284.1432 - mae: 13.0589 - val_loss: 17172.0234 - val_mse: 17172.0234 - val_mae: 102.2245\n",
      "Epoch 52/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 254.5334 - mse: 254.5334 - mae: 12.3678 - val_loss: 16824.7012 - val_mse: 16824.7012 - val_mae: 101.3484\n",
      "Epoch 53/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 220.9309 - mse: 220.9309 - mae: 11.4343 - val_loss: 16648.5723 - val_mse: 16648.5723 - val_mae: 100.8289\n",
      "Epoch 54/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 197.9111 - mse: 197.9111 - mae: 10.8587 - val_loss: 16481.9629 - val_mse: 16481.9609 - val_mae: 100.5068\n",
      "Epoch 55/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 173.8051 - mse: 173.8051 - mae: 10.1342 - val_loss: 16321.8076 - val_mse: 16321.8076 - val_mae: 99.7764\n",
      "Epoch 56/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 166.9733 - mse: 166.9733 - mae: 10.0212 - val_loss: 16160.3369 - val_mse: 16160.3369 - val_mae: 99.3590\n",
      "Epoch 57/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 143.1074 - mse: 143.1074 - mae: 9.2107 - val_loss: 15985.1699 - val_mse: 15985.1709 - val_mae: 98.8402\n",
      "Epoch 58/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 127.9734 - mse: 127.9734 - mae: 8.7236 - val_loss: 15918.7666 - val_mse: 15918.7666 - val_mae: 98.6081\n",
      "Epoch 59/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 115.8955 - mse: 115.8955 - mae: 8.1877 - val_loss: 15830.5381 - val_mse: 15830.5381 - val_mae: 98.3503\n",
      "Epoch 60/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 98.7500 - mse: 98.7500 - mae: 7.5843 - val_loss: 15693.1621 - val_mse: 15693.1621 - val_mae: 97.8032\n",
      "Epoch 61/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 85.2049 - mse: 85.2049 - mae: 7.0066 - val_loss: 15633.0723 - val_mse: 15633.0723 - val_mae: 97.6153\n",
      "Epoch 62/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 78.8888 - mse: 78.8888 - mae: 6.7815 - val_loss: 15456.2773 - val_mse: 15456.2773 - val_mae: 97.2279\n",
      "Epoch 63/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 68.1254 - mse: 68.1254 - mae: 6.2296 - val_loss: 15382.1895 - val_mse: 15382.1895 - val_mae: 96.9358\n",
      "Epoch 64/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 57.9564 - mse: 57.9564 - mae: 5.6845 - val_loss: 15287.9795 - val_mse: 15287.9795 - val_mae: 96.7145\n",
      "Epoch 65/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 54.2911 - mse: 54.2911 - mae: 5.4908 - val_loss: 15200.5020 - val_mse: 15200.5020 - val_mae: 96.3653\n",
      "Epoch 66/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 46.8158 - mse: 46.8158 - mae: 5.0752 - val_loss: 15100.4150 - val_mse: 15100.4150 - val_mae: 96.0339\n",
      "Epoch 67/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 42.5069 - mse: 42.5069 - mae: 4.8490 - val_loss: 15035.1953 - val_mse: 15035.1953 - val_mae: 95.8369\n",
      "Epoch 68/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 37.1363 - mse: 37.1363 - mae: 4.5362 - val_loss: 14983.9775 - val_mse: 14983.9775 - val_mae: 95.7342\n",
      "Epoch 69/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 34.3580 - mse: 34.3580 - mae: 4.3388 - val_loss: 14934.0723 - val_mse: 14934.0732 - val_mae: 95.5495\n",
      "Epoch 70/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 31.0115 - mse: 31.0115 - mae: 4.1004 - val_loss: 14911.2949 - val_mse: 14911.2959 - val_mae: 95.4411\n",
      "Epoch 71/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 26.2644 - mse: 26.2644 - mae: 3.7234 - val_loss: 14827.5137 - val_mse: 14827.5137 - val_mae: 95.2259\n",
      "Epoch 72/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 24.4069 - mse: 24.4069 - mae: 3.6423 - val_loss: 14785.3115 - val_mse: 14785.3115 - val_mae: 95.0351\n",
      "Epoch 73/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 20.6858 - mse: 20.6858 - mae: 3.2861 - val_loss: 14757.2861 - val_mse: 14757.2861 - val_mae: 94.9533\n",
      "Epoch 74/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 19.0113 - mse: 19.0113 - mae: 3.1324 - val_loss: 14705.1865 - val_mse: 14705.1875 - val_mae: 94.8093\n",
      "Epoch 75/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 16.8322 - mse: 16.8322 - mae: 2.9004 - val_loss: 14672.8135 - val_mse: 14672.8135 - val_mae: 94.7400\n",
      "Epoch 76/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 14.5816 - mse: 14.5816 - mae: 2.7046 - val_loss: 14667.7305 - val_mse: 14667.7305 - val_mae: 94.6074\n",
      "Epoch 77/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 13.8770 - mse: 13.8770 - mae: 2.6710 - val_loss: 14644.6572 - val_mse: 14644.6572 - val_mae: 94.5971\n",
      "Epoch 78/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 12.2002 - mse: 12.2002 - mae: 2.4557 - val_loss: 14569.1816 - val_mse: 14569.1816 - val_mae: 94.3492\n",
      "Epoch 79/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 10.9235 - mse: 10.9235 - mae: 2.2930 - val_loss: 14578.4893 - val_mse: 14578.4893 - val_mae: 94.4038\n",
      "Epoch 80/150\n",
      "13/13 [==============================] - 1s 69ms/step - loss: 10.1244 - mse: 10.1244 - mae: 2.2356 - val_loss: 14537.4590 - val_mse: 14537.4590 - val_mae: 94.2790\n",
      "Epoch 81/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 8.8662 - mse: 8.8662 - mae: 2.0574 - val_loss: 14528.1240 - val_mse: 14528.1240 - val_mae: 94.2432\n",
      "Epoch 82/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 8.7084 - mse: 8.7084 - mae: 2.0783 - val_loss: 14526.4209 - val_mse: 14526.4209 - val_mae: 94.1751\n",
      "Epoch 83/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 7.9501 - mse: 7.9501 - mae: 1.9667 - val_loss: 14435.3535 - val_mse: 14435.3535 - val_mae: 93.9699\n",
      "Epoch 84/150\n",
      "13/13 [==============================] - 1s 68ms/step - loss: 7.4332 - mse: 7.4332 - mae: 1.9442 - val_loss: 14444.4199 - val_mse: 14444.4199 - val_mae: 94.0201\n",
      "Epoch 85/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 6.1565 - mse: 6.1565 - mae: 1.6858 - val_loss: 14422.6221 - val_mse: 14422.6221 - val_mae: 93.9039\n",
      "Epoch 86/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 5.3842 - mse: 5.3842 - mae: 1.5498 - val_loss: 14420.8965 - val_mse: 14420.8965 - val_mae: 93.9013\n",
      "Epoch 87/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 5.1071 - mse: 5.1071 - mae: 1.5121 - val_loss: 14417.3447 - val_mse: 14417.3447 - val_mae: 93.8819\n",
      "Epoch 88/150\n",
      "13/13 [==============================] - 1s 70ms/step - loss: 5.2738 - mse: 5.2738 - mae: 1.5564 - val_loss: 14395.4648 - val_mse: 14395.4648 - val_mae: 93.7539\n",
      "Epoch 89/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 4.1365 - mse: 4.1365 - mae: 1.3649 - val_loss: 14391.8691 - val_mse: 14391.8691 - val_mae: 93.7954\n",
      "Epoch 90/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 3.5367 - mse: 3.5367 - mae: 1.2486 - val_loss: 14353.9854 - val_mse: 14353.9854 - val_mae: 93.6856\n",
      "Epoch 91/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 3.2988 - mse: 3.2988 - mae: 1.1603 - val_loss: 14349.3555 - val_mse: 14349.3555 - val_mae: 93.6920\n",
      "Epoch 92/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 2.9014 - mse: 2.9014 - mae: 1.1006 - val_loss: 14355.3291 - val_mse: 14355.3291 - val_mae: 93.6476\n",
      "Epoch 93/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 2.9720 - mse: 2.9720 - mae: 1.1519 - val_loss: 14324.0898 - val_mse: 14324.0918 - val_mae: 93.5739\n",
      "Epoch 94/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 2.4306 - mse: 2.4306 - mae: 1.0023 - val_loss: 14302.3281 - val_mse: 14302.3281 - val_mae: 93.5387\n",
      "Epoch 95/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 2.1638 - mse: 2.1638 - mae: 0.9542 - val_loss: 14300.9463 - val_mse: 14300.9463 - val_mae: 93.5089\n",
      "Epoch 96/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 1.9252 - mse: 1.9252 - mae: 0.8717 - val_loss: 14310.1543 - val_mse: 14310.1543 - val_mae: 93.5087\n",
      "Epoch 97/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 1.9959 - mse: 1.9959 - mae: 0.9187 - val_loss: 14273.3320 - val_mse: 14273.3320 - val_mae: 93.4432\n",
      "Epoch 98/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 1.7403 - mse: 1.7403 - mae: 0.8451 - val_loss: 14276.7559 - val_mse: 14276.7559 - val_mae: 93.4442\n",
      "Epoch 99/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 1.4811 - mse: 1.4811 - mae: 0.7520 - val_loss: 14280.6572 - val_mse: 14280.6572 - val_mae: 93.4546\n",
      "Epoch 100/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 1.3665 - mse: 1.3665 - mae: 0.7035 - val_loss: 14251.3281 - val_mse: 14251.3281 - val_mae: 93.3598\n",
      "Epoch 101/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 1.2251 - mse: 1.2251 - mae: 0.6705 - val_loss: 14262.9346 - val_mse: 14262.9346 - val_mae: 93.4120\n",
      "Epoch 102/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 1.0788 - mse: 1.0788 - mae: 0.6321 - val_loss: 14260.5957 - val_mse: 14260.5957 - val_mae: 93.3674\n",
      "Epoch 103/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 1.1320 - mse: 1.1320 - mae: 0.6713 - val_loss: 14241.3936 - val_mse: 14241.3936 - val_mae: 93.3402\n",
      "Epoch 104/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 1.0796 - mse: 1.0796 - mae: 0.6547 - val_loss: 14245.6875 - val_mse: 14245.6875 - val_mae: 93.3301\n",
      "Epoch 105/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 0.8733 - mse: 0.8733 - mae: 0.5567 - val_loss: 14226.3135 - val_mse: 14226.3135 - val_mae: 93.2882\n",
      "Epoch 106/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.7365 - mse: 0.7365 - mae: 0.4987 - val_loss: 14235.5859 - val_mse: 14235.5869 - val_mae: 93.3015\n",
      "Epoch 107/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 0.7798 - mse: 0.7798 - mae: 0.5432 - val_loss: 14213.3760 - val_mse: 14213.3760 - val_mae: 93.2502\n",
      "Epoch 108/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.7935 - mse: 0.7935 - mae: 0.5787 - val_loss: 14227.1846 - val_mse: 14227.1846 - val_mae: 93.2679\n",
      "Epoch 109/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.6175 - mse: 0.6175 - mae: 0.4811 - val_loss: 14220.6055 - val_mse: 14220.6055 - val_mae: 93.2489\n",
      "Epoch 110/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.5524 - mse: 0.5524 - mae: 0.4474 - val_loss: 14211.9551 - val_mse: 14211.9551 - val_mae: 93.2458\n",
      "Epoch 111/150\n",
      "13/13 [==============================] - 1s 71ms/step - loss: 0.4982 - mse: 0.4982 - mae: 0.3971 - val_loss: 14213.6699 - val_mse: 14213.6699 - val_mae: 93.2345\n",
      "Epoch 112/150\n",
      "13/13 [==============================] - 1s 72ms/step - loss: 0.4475 - mse: 0.4475 - mae: 0.3699 - val_loss: 14202.4395 - val_mse: 14202.4395 - val_mae: 93.2020\n",
      "Epoch 113/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.3987 - mse: 0.3987 - mae: 0.3466 - val_loss: 14206.6299 - val_mse: 14206.6299 - val_mae: 93.2166\n",
      "Epoch 114/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.3656 - mse: 0.3656 - mae: 0.3397 - val_loss: 14206.6611 - val_mse: 14206.6611 - val_mae: 93.2102\n",
      "Epoch 115/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.3518 - mse: 0.3518 - mae: 0.3206 - val_loss: 14185.2529 - val_mse: 14185.2529 - val_mae: 93.1601\n",
      "Epoch 116/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.3352 - val_loss: 14197.7822 - val_mse: 14197.7822 - val_mae: 93.1903\n",
      "Epoch 117/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.3289 - mse: 0.3289 - mae: 0.3213 - val_loss: 14198.2637 - val_mse: 14198.2637 - val_mae: 93.1864\n",
      "Epoch 118/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.2763 - mse: 0.2763 - mae: 0.2977 - val_loss: 14186.0273 - val_mse: 14186.0273 - val_mae: 93.1519\n",
      "Epoch 119/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.2396 - mse: 0.2396 - mae: 0.2708 - val_loss: 14186.2637 - val_mse: 14186.2637 - val_mae: 93.1571\n",
      "Epoch 120/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.2124 - mse: 0.2124 - mae: 0.2537 - val_loss: 14188.8643 - val_mse: 14188.8643 - val_mae: 93.1580\n",
      "Epoch 121/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.2251 - mse: 0.2251 - mae: 0.2580 - val_loss: 14187.4248 - val_mse: 14187.4248 - val_mae: 93.1519\n",
      "Epoch 122/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.2169 - mse: 0.2169 - mae: 0.2823 - val_loss: 14174.6846 - val_mse: 14174.6846 - val_mae: 93.1278\n",
      "Epoch 123/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.1826 - mse: 0.1826 - mae: 0.2443 - val_loss: 14180.1309 - val_mse: 14180.1299 - val_mae: 93.1353\n",
      "Epoch 124/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.1585 - mse: 0.1585 - mae: 0.2282 - val_loss: 14183.1064 - val_mse: 14183.1064 - val_mae: 93.1344\n",
      "Epoch 125/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.1556 - mse: 0.1556 - mae: 0.2152 - val_loss: 14175.9756 - val_mse: 14175.9756 - val_mae: 93.1220\n",
      "Epoch 126/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.1417 - mse: 0.1417 - mae: 0.2101 - val_loss: 14176.4492 - val_mse: 14176.4492 - val_mae: 93.1141\n",
      "Epoch 127/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.1149 - mse: 0.1149 - mae: 0.1762 - val_loss: 14172.8906 - val_mse: 14172.8906 - val_mae: 93.1095\n",
      "Epoch 128/150\n",
      "13/13 [==============================] - 1s 79ms/step - loss: 0.0994 - mse: 0.0994 - mae: 0.1566 - val_loss: 14170.7725 - val_mse: 14170.7725 - val_mae: 93.0955\n",
      "Epoch 129/150\n",
      "13/13 [==============================] - 1s 77ms/step - loss: 0.1006 - mse: 0.1006 - mae: 0.1704 - val_loss: 14166.8564 - val_mse: 14166.8564 - val_mae: 93.0951\n",
      "Epoch 130/150\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 0.1077 - mse: 0.1077 - mae: 0.2035 - val_loss: 14167.6396 - val_mse: 14167.6396 - val_mae: 93.0922\n",
      "Epoch 131/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.1040 - mse: 0.1040 - mae: 0.1879 - val_loss: 14170.7031 - val_mse: 14170.7031 - val_mae: 93.1032\n",
      "Epoch 132/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0955 - mse: 0.0955 - mae: 0.1791 - val_loss: 14167.7217 - val_mse: 14167.7217 - val_mae: 93.0950\n",
      "Epoch 133/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.0848 - mse: 0.0848 - mae: 0.1561 - val_loss: 14163.4609 - val_mse: 14163.4609 - val_mae: 93.0844\n",
      "Epoch 134/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0762 - mse: 0.0762 - mae: 0.1530 - val_loss: 14164.0830 - val_mse: 14164.0830 - val_mae: 93.0797\n",
      "Epoch 135/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.0595 - mse: 0.0595 - mae: 0.1351 - val_loss: 14162.9277 - val_mse: 14162.9277 - val_mae: 93.0780\n",
      "Epoch 136/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.0546 - mse: 0.0546 - mae: 0.1139 - val_loss: 14161.0146 - val_mse: 14161.0146 - val_mae: 93.0737\n",
      "Epoch 137/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0553 - mse: 0.0553 - mae: 0.1395 - val_loss: 14159.3701 - val_mse: 14159.3701 - val_mae: 93.0688\n",
      "Epoch 138/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.0470 - mse: 0.0470 - mae: 0.1152 - val_loss: 14158.9141 - val_mse: 14158.9150 - val_mae: 93.0672\n",
      "Epoch 139/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0499 - mse: 0.0499 - mae: 0.1235 - val_loss: 14160.8613 - val_mse: 14160.8613 - val_mae: 93.0694\n",
      "Epoch 140/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0515 - mse: 0.0515 - mae: 0.1426 - val_loss: 14160.6436 - val_mse: 14160.6436 - val_mae: 93.0704\n",
      "Epoch 141/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0467 - mse: 0.0467 - mae: 0.1387 - val_loss: 14157.9814 - val_mse: 14157.9814 - val_mae: 93.0650\n",
      "Epoch 142/150\n",
      "13/13 [==============================] - 1s 73ms/step - loss: 0.0326 - mse: 0.0326 - mae: 0.1049 - val_loss: 14157.7109 - val_mse: 14157.7109 - val_mae: 93.0611\n",
      "Epoch 143/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0283 - mse: 0.0283 - mae: 0.0876 - val_loss: 14158.4082 - val_mse: 14158.4082 - val_mae: 93.0630\n",
      "Epoch 144/150\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.0732 - val_loss: 14155.7773 - val_mse: 14155.7773 - val_mae: 93.0559\n",
      "Epoch 145/150\n",
      "13/13 [==============================] - 1s 78ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.0716 - val_loss: 14156.2363 - val_mse: 14156.2363 - val_mae: 93.0558\n",
      "Epoch 146/150\n",
      "13/13 [==============================] - 1s 76ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.0689 - val_loss: 14154.5283 - val_mse: 14154.5283 - val_mae: 93.0516\n",
      "Epoch 147/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.0812 - val_loss: 14152.7354 - val_mse: 14152.7354 - val_mae: 93.0464\n",
      "Epoch 148/150\n",
      "13/13 [==============================] - 1s 75ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.0927 - val_loss: 14152.7793 - val_mse: 14152.7793 - val_mae: 93.0422\n",
      "Epoch 149/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0262 - mse: 0.0262 - mae: 0.0979 - val_loss: 14152.6279 - val_mse: 14152.6279 - val_mae: 93.0468\n",
      "Epoch 150/150\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0835 - val_loss: 14153.3047 - val_mse: 14153.3047 - val_mae: 93.0439\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "history=model.fit(X_train, y_train, batch_size=80, epochs=150, verbose=1, validation_split=0.2)\n",
    "#predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try a second approach, we replace the TF-IDF by the Doc2Vec, a deep learning approach that computes a vector for each text. Thanks to these vectors, called embeddings, we can compare whole documents together. We will use this method and see if it outperforms the TF-IDF approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same methods as before, hence only the results will be discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gensim' from 'C:\\\\Users\\\\hugop\\\\anaconda3\\\\lib\\\\site-packages\\\\gensim\\\\__init__.py'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    " def LL_to_TGdoc(file=list(df_fr['Text']), tokens_only=False):\n",
    "    \n",
    "    for i, tokens in enumerate(file):\n",
    "        if tokens_only:\n",
    "                yield tokens\n",
    "        else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "        \n",
    "train_corpus = list(LL_to_TGdoc())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model = Doc2Vec(train_corpus, vector_size=500, window=8, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1285"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.docvecs.doctag_syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.docvecs.doctag_syn0\n",
    "a = a.tolist()\n",
    "\n",
    "X_train = a[:1000]\n",
    "y_train = list(df_fr['date'])[:1000]\n",
    "X_test = a[1000:]\n",
    "y_test = list(df_fr['date'])[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for item in y_train:\n",
    "    a.append(float(item))\n",
    "    \n",
    "y_train = a\n",
    "\n",
    "\n",
    "a = []\n",
    "for item in y_test:\n",
    "    a.append(float(item))\n",
    "    \n",
    "y_test = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier (as baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy of 7.36% with the Dummy Classifier. This score being lower than the one we got with the Dummy on the TF-IDF data, we can expect the Doc2Vec approach to be worse overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07368421052631578"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Setup arrays to store training and test accuracies\n",
    "neighbors = np.arange(1,40,2)\n",
    "train_accuracy =np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXxU1f34/9c7O1lIQgiLBBIXEEUIYgQXFBCt4r4vta1aW8XWpfrwV221fmht+22tfqx+WrVqrdpacKvWWtSKQnFX3AVlEQMEEEIgQAhkff/+ODfJzWSSTEImM+S+n4/HPGbuOu+5k9z3nHPuOVdUFWOMMcGVEOsAjDHGxJYlAmOMCThLBMYYE3CWCIwxJuAsERhjTMBZIjDGmICzRLCHEZFSETk21nH0FhFZLCJTYx3H7hCRi0Xk9Ri+/xUiskFEqkQkrwf3O8LbZ2IE6xaJiIpIUjvLZ4nI33oqNtM1lgj6MO+fS0XkHN+8JG9ekTf9sDc90bfOfiIStoOJiLwkIr8IM/80Efm6vX/07lLVMaq6oCf3GSQikgz8L/ANVc1U1Yqe2reqrvb22dBT+zSxYYmg79sM/KKTX22bgV9GuL+HgW+LiITM/zbwmKrWRxpYTyeNIOjGMRsMpAGLoxBOXIqkhGJas0SwBxOR0SLylYic38FqLwK1wLc6WOcRYJyITIngbZ8FBgBH+eLIBU4GHhWRiSLylohUish6EfmDiKT41lUR+aGILAeWi8gfReSOkM/1LxH5kfe6uSrMK+E8ISKPish2r9qoxLfdBBH50Fv2pIg8LiJhE1xTdY2I3C4iW7zjOMO3vFUVnL/qwlfNcYmIrPG2nykih4rIJ95n/0Pbt5T/E5GtIvKFiEz3LcgWkT97x2utiPyy6WTmxfmGiNwpIpuBWWE+S6qI/F5E1nmP33vzRgFLvdUqReTVMNs2fZaLRGS1iGwSkZt8yxNE5EYR+VJEKrzjPyBk2yRvem8RWegd/3nedxta3XNhuPfxpHnf2XYR+UBEin1xHCAiC7xju1hETvUte1hE7hWRuSKyA5gmIieKyBJvX2tF5PrQz258VNUee9ADKAWOBSYAq4GTO1h3FvA34FRgJZAMJAEKFHnrPIwrDVwNvO7N28/9abS73weAB33TlwMfea8PAQ7z3qcI+Bz4kW9dBV7GJZN+wERgHZDgLR8IVAOD/Z/X93l2AScCicD/A972lqUAq4BrvM95Ji4B/rKdz3AxUAd839vXFV4cEvq+/mPpvS7yPsd9uF/b3/DiehYYBAwDNgJTfO9VD1zrxXYesBUY4C1/FvgTkOFt/y5weci2V3nHtF+Yz/IL4G1v23zgTeDWkFiT2jkOTcsf8L6PYqAGOMBb/iNv3wVAqhfn7HD7Bt4Cbve+i8nAtjDHrL33meV9H2d7x+h64CvvdTKwAvipt+9jgO3A/r6/4a3Akbgft2nAeuAob3kuMCHW/7vx/Ih5APbo4hfmTlA/B8qAaZ2s6z95vYM72bWXCFJxiWUGnSeCyd4/Xj9v+g3g2nbW/RHwjG9agWNC1vkcOM57fSUwN+Tz+hPBPN+yA4Gd3uujgbV4J3Jv3ut0nAhW+KbTvdiGhL5vmGPZdFIb5lteAZznm34aLwF679WcZLx57+Kq0wZ7J8R+vmUXAPN9267u5Hv+EjjRN308UBoSa2eJoCAktvN9381037KhuBN2kn/fwAhcwkr3rfu3MMesvfeZhZfUvekEvJO59/ga78eCt3w2MMv3N/xoyOdajfuB0j/W/7N7wsOqhvZMM4E3VXV+0wwRuVDcFRxVIvJCmG1uBm7C/VpqQ1VrgFu9R2j9f+i6rwPlwGkisg9wKPB3L45RIvK8uIbjbcCvcb/y/daETD9CS9XVt4C/dvD2X/teV+OqE5KAvYC16p0F2nmfdvelqtXey8xOtvHb4Hu9M8y0f1+hsa3CxVyI+8W73qv2qMT96h7kW7ezz7GXt7/QfXdF6HFtir0QeMYX2+dAAy6Bhcaw2Xcc24u7vfdptb6qNuJ+7OzlPdZ485qswpW82nuvs3Alx1Ui8l8ROTxMLMZjiWDPNBMYISJ3Ns1Q1cfUXcGRqaozQjdQ1ZdxxesfdLDfvwDZwBkRxPAo8B3cr9r/qGrTSfBe4AtgpKr2xxXnQxNL6BVJf8MllWLgAFxVSVetB4aJtGrEHt6N/TTZgSslNBmyG/uCtrGNwJUS1uBKBANVNcd79FfVMb51OxsieB3uhB26756wBpjhiy1HVdNUdW3IeuuBASLiP2ZdPf7N64tIAq46ap33GO7NazICVwJs0uoYqep7qnoaLqE+CzzRxVgCxRLBnmk7cAJwtIj8pgvb3QT8uL2F6q74mQXcEMG+HsW1VXwf94u+SRaubrhKREbjqqM6pKplwHu4ksDTqrozgvcP9Rbul+qV4i6RPQ3X/tBdHwHni0iy1yB99m7sC9wJ6Wpvf+fgEt5cVV0P/Ae4Q0T6e42z+0pkDfdNZgM3i0i+iAwEbsEl155wH/ArESkE8N7jtNCVVHUVsAiYJSIp3i/wU7r4XoeIyJleCe9HuAT5Nq5acwfwY+/4TfX2PSfcTrz3v1BEslW1Dvf3aJe4dsASwR5KVSuB44AZInJrhNu8gauX7chs3K+7zvZVimuUzACe8y26HvgmLlk9ADweSWy4ZDKWjquFOoqnFtdAfClQiatieh53MumOnwH7AltwbTJ/7+Z+mrwDjAQ2Ab8CztaWa/q/g2sEXeK931O4uvhI/RJ3Ev4E+BT4gMgvB+7MXbjv9z8ish13Yp7UzroXAofj2kt+ifvuu3L8/4lrSN+CK2meqap13nd7Kq79ahNwD/AdVf2ig319Gyj1qidn0vFVc4HXdIWEMTElIkfjfsUWhdQF784+3wHuU9W/9MT+TNeIyOPAF6r6P7GOxXTMSgQm5sT1fr0Gd0lqt5OAiEwRkSFe1dBFwDhcPwrTC8T1o9jXq946ATiN7rX3mF5mPTtNTInIAbhqjY+BS3Zzd/vjGgUzcZdUnu3VwZveMQT4B5CHu+LnClX9MLYhmUhY1ZAxxgScVQ0ZY0zA7XFVQwMHDtSioqJYh2GMMXuU999/f5Oq5odbtsclgqKiIhYtWhTrMIwxZo8iIqvaW2ZVQ8YYE3CWCIwxJuAsERhjTMDtcW0Expiuq6uro6ysjF27dsU6FBNlaWlpFBQUkJycHPE2lgiMCYCysjKysrIoKipC2txl1PQVqkpFRQVlZWXsvffeEW8XtaohEXlIRDaKyGftLBcRuVtEVoi7vd+EaMViTNDt2rWLvLw8SwJ9nIiQl5fX5ZJfNNsIHsYNldyeGbjRGEcCl+HGsTfGRIklgWDozvcctUSgqguBzR2schru9nKqqm8DOSLSlaF3u2bDYpg3C3ZWRu0tjDFmTxTLq4aG0fr2cmW0vvVcMxG5TEQWicii8vLy7r3bllJ4/U7Y/GX3tjfGdFtFRQXjx49n/PjxDBkyhGHDhjVP19bWdrr9ggULePPNN5un77vvPh599NEei6+8vJzk5GT+9Kc/9dg+9ySxbCwOV34JOwKeqt4P3A9QUlLSvVHycrw7+W1ZBcMO6dYujDHdk5eXx0cffQTArFmzyMzM5Prrr494+wULFpCZmckRRxwBwMyZM3s0vieffJLDDjuM2bNnc/nll/fovv3q6+tJSoq/a3RiWSIoo/U9TZvuTxoduV4iqGy3l7Uxphe9//77TJkyhUMOOYTjjz+e9evdiOF33303Bx54IOPGjeP888+ntLSU++67jzvvvJPx48fz2muvMWvWLG6//XYApk6dyg033MDEiRMZNWoUr732GgDV1dWce+65jBs3jvPOO49Jkya1OzzN7NmzueOOOygrK2Pt2pZbIT/66KOMGzeO4uJivv3tbwOwYcMGzjjjDIqLiykuLubNN9+ktLSUgw46qHm722+/nVmzZjXH99Of/pQpU6Zw11138a9//YtJkyZx8MEHc+yxx7Jhg7vdd1VVFZdccgljx45l3LhxPP300/z5z3/m2muvbd7vAw88wHXXXddD30CLWKam53D3l52Du/Xd1qiOHZ+aBf0GuBKBMQH2838tZsm6bT26zwP36s//nDIm4vVVlauuuop//vOf5Ofn8/jjj3PTTTfx0EMP8Zvf/IavvvqK1NRUKisrycnJYebMma1KEa+88kqr/dXX1/Puu+8yd+5cfv7znzNv3jzuuececnNz+eSTT/jss88YP3582FjWrFnD119/zcSJEzn33HN5/PHHue6661i8eDG/+tWveOONNxg4cCCbN7smz6uvvpopU6bwzDPP0NDQQFVVFVu2bOnw81ZWVvLf//4XgC1btvD2228jIjz44IPcdttt3HHHHdx6661kZ2fz6aefNq+XkpLCuHHjuO2220hOTuYvf/lLVKqvopYIRGQ2MBUYKCJlwP8AyQCqeh8wFzgRWAFUs/s3JelcbqGVCIyJAzU1NXz22Wccd9xxADQ0NDB0qLtWZNy4cVx44YWcfvrpnH766RHt78wzzwTgkEMOobS0FIDXX3+da665BoCDDjqIcePGhd12zpw5nHvuuQCcf/75XHrppVx33XW8+uqrnH322QwcOBCAAQMGAPDqq682t08kJiaSnZ3daSI477zzml+XlZVx3nnnsX79empra5uv9583bx5z5sxpXi83NxeAY445hueff54DDjiAuro6xo4dG9Ex6YqoJQJVvaCT5Qr8MFrvH1ZOIXz9aa++pTHxpiu/3KNFVRkzZgxvvfVWm2X//ve/WbhwIc899xy33norixcv7nR/qampgDsx19fXN79HJGbPns2GDRt47LHHAFi3bh3Lly9HVSO+FDMpKYnGxpa7rIZex5+RkdH8+qqrruK6667j1FNPZcGCBc1VSO293/e+9z1+/etfM3r0aC65JDq/l4M11lBuIWxdA409cm90Y0w3paamUl5e3pwI6urqWLx4MY2NjaxZs4Zp06Zx2223UVlZSVVVFVlZWWzfvr1L7zF58mSeeOIJAJYsWdJc5eK3dOlSduzYwdq1ayktLaW0tJSf/OQnzJkzh+nTp/PEE09QUVEB0Fw1NH36dO6913V7amhoYNu2bQwePJiNGzdSUVFBTU0Nzz//fLtxbd26lWHD3AWSjzzySPP8b3zjG/zhD39onm4qZUyaNIk1a9bw97//nQsu6PD3dbcFKxHkFEJDLWy329gaE0sJCQk89dRT3HDDDRQXFzN+/HjefPNNGhoa+Na3vsXYsWM5+OCDufbaa8nJyeGUU07hmWeeaW4sjsQPfvADysvLGTduHL/97W8ZN24c2dnZrdaZPXs2Z5xxRqt5Z511FrNnz2bMmDHcdNNNTJkyheLi4uZG2rvuuov58+czduxYDjnkEBYvXkxycjK33HILkyZN4uSTT2b06NHtxjVr1izOOeccjjrqqOZqJ4Cbb76ZLVu2cNBBB1FcXMz8+fObl5177rkceeSRzdVFPW2Pu2dxSUmJdvvGNCvmwd/OgktegMIjejYwY+LY559/zgEHHBDrMHpVQ0MDdXV1pKWl8eWXXzJ9+nSWLVtGSkpKrEPrspNPPplrr72W6dOnR7R+uO9bRN5X1ZJw68ffBa3RlFPknresskRgTB9XXV3NtGnTqKurQ1W5995797gkUFlZycSJEykuLo44CXRHwBLBcEDsyiFjAiArK2uPv61tTk4Oy5Yti/r7BKuNICkVsoZaXwJjjPEJViIA60tgjDEhgpcIcgqtRGCMMT7BSwS5hbBtLdR3PuKhMcYEQfASQU4hoK5jmTGmV+zOMNSLFi3i6quv7vQ9mkYmNV0XrKuGoPUopHn7xjYWYwKis2GoOxqeuaSkhJKSsJe/t+K/X8GeoqGhgcTExFiHEdQSAdZOYEyMXXzxxVx33XVMmzaNG264gXfffZcjjjiCgw8+mCOOOIKlS5cC7l4EJ598MuCSyHe/+12mTp3KPvvsw9133928v8zMzOb1p06dytlnn83o0aO58MILm8cdmjt3LqNHj2by5MlcffXVzfv1Ky0t5aijjmLChAlMmDChVYK57bbbGDt2LMXFxdx4440ArFixgmOPPZbi4mImTJjAl19+2SpmgCuvvJKHH34YgKKiIn7xi18wefJknnzySR544AEOPfRQiouLOeuss6iurgbCD3f9s5/9jLvuuqt5vzfddFOrY9BdwSsR9N8LEpLtyiETXC/c2PODLw4ZCzN+0+XNli1bxrx580hMTGTbtm0sXLiQpKQk5s2bx09/+lOefvrpNtt88cUXzJ8/n+3bt7P//vtzxRVXkJyc3GqdDz/8kMWLF7PXXntx5JFH8sYbb1BSUsLll1/OwoUL2Xvvvdsdt2fQoEG8/PLLpKWlsXz5ci644AIWLVrECy+8wLPPPss777xDenp689hDF154ITfeeCNnnHEGu3btah4vqSNpaWm8/vrrgKs2+/73vw+4YSb+/Oc/c9VVV4Ud7nqvvfbizDPP5JprrqGxsZE5c+bw7rvvdvm4hwpeIkhIhOwCKxEYEwfOOeec5qqRrVu3ctFFF7F8+XJEhLq6urDbnHTSSaSmppKamsqgQYPYsGEDBQUFrdaZOHFi87zx48dTWlpKZmYm++yzT/OwzxdccAH3339/m/3X1dVx5ZVX8tFHH5GYmNjcoWvevHlccsklpKenA25Y6u3bt7N27drm8YrS0tIi+tz+Yak/++wzbr755uYB9o4//ngg/HDX2dnZ5OXl8eGHH7JhwwYOPvhg8vLyInrPjgQvEYD1JTDB1o1f7tHiH575Zz/7GdOmTeOZZ56htLSUqVOnht2machpaD3sdGfrRDqu2p133sngwYP5+OOPaWxsbD65hxsmur19dmVY6osvvphnn32W4uJiHn74YRYsWNBhfN/73vd4+OGH+frrr/nud78b0WfqTPDaCMD6EhgTh/zDMzfVp/ek0aNHs3LlyuYb1zz++OPtxjF06FASEhL461//SkNDA+CGiX7ooYea6/A3b95M//79KSgo4NlnnwXcDXeqq6spLCxkyZIl1NTUsHXr1jZ3VPPbvn07Q4cOpa6urvmeCBB+uGuAM844gxdffJH33nuvufSwu4KZCHILoXoT1FTFOhJjjOfHP/4xP/nJTzjyyCObT749qV+/ftxzzz2ccMIJTJ48mcGDB7cZlhrc8NWPPPIIhx12GMuWLWv+9X7CCSdw6qmnUlJSwvjx45vvmfzXv/6Vu+++m3HjxnHEEUfw9ddfM3z48Ob7JV944YUcfPDB7cZ16623MmnSJI477rhWw1eHG+4aICUlhWnTpnHuuef22BVHwRqGusmnT8HTl8IVb8HgA3smMGPiWBCHoQ6nqqqKzMxMVJUf/vCHjBw5stXN4fcEjY2NTJgwgSeffJKRI0eGXaerw1AHtERQ5J6tncCYQHnggQcYP348Y8aMYevWrVx++eWxDqlLlixZwn777cf06dPbTQLdEczGYutLYEwgXXvttXtcCcDvwAMPZOXKlT2+32CWCDIGQnK6lQhMoOxp1cCme7rzPQczEYjYlUMmUNLS0qioqLBk0MepKhUVFRH3Z2gSzKohcFcObSmNdRTG9IqCggLKysooLy+PdSgmytLS0tp0sOtMcBNBTiGUvg6qroRgTB+WnJzc3KPWmFDBrBoCVyKorYLqzbGOxBhjYiq4iaDpyqHK0piGYYwxsRbcRJBrl5AaYwwEORHk+G5QY4wxARbcRJDWH/rlWonAGBN4wU0E4EoFViIwxgRcsBNBrnUqM8aYYCeCnELYugZ8N5AwxpigCXYiyC2EhlrYvj7WkRhjTMxENRGIyAkislREVojIjWGWZ4vIv0TkYxFZLCKXRDOeNnKK3LO1ExhjAixqiUBEEoE/AjOAA4ELRCT0LjA/BJaoajEwFbhDRFKiFVMb1pfAGGOiWiKYCKxQ1ZWqWgvMAU4LWUeBLHF3hM4ENgNt70QdLdnD3bOVCIwxARbNRDAMWOObLvPm+f0BOABYB3wKXKOqbVpuReQyEVkkIot6dPTE5DTIGmolAmNMoEUzEYQb0jN0MPTjgY+AvYDxwB9EpH+bjVTvV9USVS3Jz8/v2SitL4ExJuCimQjKgOG+6QLcL3+/S4B/qLMC+AoYHcWY2rK+BMaYgItmIngPGCkie3sNwOcDz4WssxqYDiAig4H9gZ6/IWdHcgph21qor+3VtzXGmHgRtUSgqvXAlcBLwOfAE6q6WERmishMb7VbgSNE5FPgFeAGVd0UrZjCyi0E1HUsM8aYAIrqHcpUdS4wN2Tefb7X64BvRDOGTvlHIc3bN6ahGGNMLAS7ZzFYXwJjTOBZIug/DBKS7MohY0xgWSJISHQdy6xEYIwJKEsE4KqHrERgjAkoSwTgGoytRGCMCShLBOBKBNWboKYq1pEYY0yvs0QAvktIV8c2DmOMiQFLBAC5Re7Z2gmMMQFkiQBaSgTWTmCMCSBLBAAZAyE53UoExphAskQAIGJXDhljAssSQRPrS2CMCShLBE2aSgQaeu8cY4zp2ywRNMkthNrtsHNLrCMxxpheZYmgSfOVQ6UxDcMYY3qbJYImub77EhhjTIBYImhifQmMMQFliaBJWn/ol2slAmNM4Fgi8LO+BMaYALJE4Gd9CYwxAWSJwC+n0I1A2tgY60iMMabXWCLwyy2Ehlqo+jrWkRhjTK+xROCXU+SerZ3AGBMglgj8rC+BMSaALBH4ZQ93z1YiMMYEiCUCv+Q0yBpqJQJjTKBYIghlfQmMMQFjiSCU9SUwxgSMJYJQOYWwbS001MU6EmOM6RWWCELlFoI2wtY1sY7EGGN6hSWCUDYKqTEmYDpNBCJysogEJ2FYXwJjTMBEcoI/H1guIreJyAHRDijm+g+DhCQrERhjAqPTRKCq3wIOBr4E/iIib4nIZSKS1dm2InKCiCwVkRUicmM760wVkY9EZLGI/LfLn6CnJSRCdoGVCIwxgRFRlY+qbgOeBuYAQ4EzgA9E5Kr2thGRROCPwAzgQOACETkwZJ0c4B7gVFUdA5zTnQ/R46wvgTEmQCJpIzhFRJ4BXgWSgYmqOgMoBq7vYNOJwApVXamqtbgkclrIOt8E/qGqqwFUdWM3PkPPs74ExpgASYpgnXOAO1V1oX+mqlaLyHc72G4Y4L8GswyYFLLOKCBZRBYAWcBdqvpo6I5E5DLgMoARI0ZEEPJuyimEHeVQuwNSMqL/fsYYE0ORVA39D/Bu04SI9BORIgBVfaWD7STMPA2ZTgIOAU4Cjgd+JiKj2myker+qlqhqSX5+fgQh76bcIvdcuTr672WMMTEWSSJ4EvDfsqvBm9eZMmC4b7oAWBdmnRdVdYeqbgIW4qqcYsv6EhhjAiSSRJDk1fED4L1OiWC794CRIrK3iKTgLkN9LmSdfwJHiUiSiKTjqo4+jyz0KLK+BMaYAIkkEZSLyKlNEyJyGrCps41UtR64EngJd3J/QlUXi8hMEZnprfM58CLwCa766UFV/azrH6OHZeRDcrqVCIwxgRBJY/FM4DER+QOu3n8N8J1Idq6qc4G5IfPuC5n+HfC7iKLtLSKQM8JKBMaYQOg0Eajql8BhIpIJiKpuj35YccD6EhhjAiKSEgEichIwBkgTcRcDqeovohhX7OUWwuq3QNWVEIwxpo+KpEPZfcB5wFW4qqFzgMIoxxV7OYVQsw12bol1JMYYE1WRNBYfoarfAbao6s+Bw2l9WWjfZFcOGWMCIpJEsMt7rhaRvYA6YO/ohRQnrC+BMSYgImkj+Jc3ONzvgA9wvYMfiGpU8cBKBMaYgOgwEXg3pHlFVSuBp0XkeSBNVbf2SnSxlJYNaTlWIjDG9HkdVg2paiNwh2+6JhBJoImNQmqMCYBI2gj+IyJniQTwGkrrS2CMCYBI2giuAzKAehHZhbuEVFW1f1Qjiwe5hbDsJWhshITg3LbZGBMskfQs7vSWlH1WTiE01EDVBug/NNbRGGNMVHSaCETk6HDzQ29U0yc135dglSUCY0yfFUnV0P/ne52GuwXl+8AxUYkonvj7Eow4LLaxGGNMlERSNXSKf1pEhgO3RS2ieJLj3RbTrhwyxvRh3WkBLQMO6ulA4lJyGmQOsSuHjDF9WiRtBP9Hy72GE4DxwMfRDCquWF8CY0wfF0kbwSLf63pgtqq+EaV44k9OIax+O9ZRGGNM1ESSCJ4CdqlqA4CIJIpIuqpWRze0OJFbCJ89BQ11kJgc62iMMabHRdJG8ArQzzfdD5gXnXDiUE4haCNsLYt1JMYYExWRJII0Va1qmvBep0cvpDhjo5AaY/q4SBLBDhGZ0DQhIocAO6MXUpyx+xIYY/q4SNoIfgQ8KSLrvOmhuFtXBkP/YSCJViIwxvRZkXQoe09ERgP74wac+0JV66IeWbxITILsAisRGGP6rEhuXv9DIENVP1PVT4FMEflB9EOLI9aXwBjTh0XSRvB97w5lAKjqFuD70QspDtl9CYwxfVgkiSDBf1MaEUkEUqIXUhzKLYQdG6E2GF0njDHBEkkieAl4QkSmi8gxwGzgheiGFWdyitxz5eqYhmGMMdEQSSK4Adep7Argh8AntO5g1vdZXwJjTB/WaSLwbmD/NrASKAGmA59HOa74Yn0JjDF9WLuXj4rIKOB84AKgAngcQFWn9U5ocSRzECT1sxKBMaZP6qgfwRfAa8ApqroCQESu7ZWo4o2Iqx7aUhrrSIwxpsd1VDV0FvA1MF9EHhCR6bgOZcGUY30JjDF9U7uJQFWfUdXzgNHAAuBaYLCI3Csi3+il+OJHbiFssauGjDF9TySNxTtU9TFVPRkoAD4Cboxk5yJygogsFZEVItLuNiJyqIg0iMjZEUfe23IKoWYr7NwS60iMMaZHdemexaq6WVX/pKrHdLau1/Hsj8AM4EDgAhE5sJ31fovrrxC/Bo5yz589Hds4jDGmh3Xn5vWRmgisUNWVqloLzAFOC7PeVcDTwMYoxrL79jsW9p0OL/4E1n0Y62iMMabHRDMRDAPW+KbLvHnNRGQYcAZwX0c7EpHLRGSRiCwqLy/v8UAjkpAAZz4AGYPgie9A9ebYxGGMMT0smokg3BVGGjL9e+CGpvsht6cpstcAABiESURBVEdV71fVElUtyc/P77EAuywjD859FLath2cuh8bG2MVijDE9JJqJoAwY7psuANaFrFMCzBGRUuBs4B4ROT2KMe2+gkPghP8Hy/8Dr98R62iMMWa3RXKHsu56DxgpInsDa3G9lL/pX0FV9256LSIPA8+r6rNRjKlnHPo9WP02zP81FBwK+0yNdUTGGNNtUSsRqGo9cCXuaqDPgSdUdbGIzBSRmdF6314hAqfcBXkj4alLYVtoQccYY/YcohpabR/fSkpKdNGiRbEOwylfCvdPgyEHwcX/hsTkWEdkjDFhicj7qloSblk02wj6vvz94bT/gzXvwMu3xDoaY4zpFksEu+ugs2DSTHj7Hlj8TKyjMcaYLrNE0BOOuxUKJsI/r4RNy2MdjTHGdIklgp6QlALnPAxJqfD4t6F2R6wjMsaYiFki6CnZw+CsB6H8C3j+WtjDGuGNMcFliaAn7XsMTPspfPI4LHoo1tEYY0xELBH0tKOuh/2OgxdvhLUfxDoaY4zplCWCnpaQAGfeD5mD4YmLbHA6Y0zcs0QQDekD4NxHoOpr+MdlNjidMSauWSKIlmHe4HQrXobXbo91NMYY0y5LBNFUcimMPdcNTvflq7GOxhhjwrJEEE0icMrvIX80PP092FoW64iMMaYNSwTRlpIB5/0V6mvgyYuhpirWERljTCuWCHrDwJFw2h+hbBHcezh8tTDWERljTDNLBL1lzOlwyQuQkASPnAL/vt5KB8aYuGCJoDcVHg4z34BJV8B7D8K9R0Dp67GOyhgTcJYIeltKOsz4DVwyFyQBHj4J5v7YBqozxsSMJYJYKTwCrngDJl4O7/4J7j0SSt+IdVTGmACyRBBLKRlw4m3uNpeoKx28cCPUVsc6MmNMgFgiiAdFk13bwaHfg3fuhfuOhFVvxToqY0xAWCKIF6mZcNLtcNG/oLEe/jIDXvyplQ6MMVFniSDe7H00XPEWHHopvP1HuG8yrH4n1lEZY/owSwTxKDUTTroDvvMcNNTBQ8fDSzdB3c5YR2aM6YMsEcSzfabAD96EkkvgrT+4K4te+1/Y+LndCtMY02NE97ATSklJiS5atCjWYfS+L+fDvFmw/iM3nVsEo2bA/idA4ZGQmBzL6IwxcU5E3lfVknDLkno7GNNN+05zj23rYNmLsPQFd1/kd+6F1GzYbzrsPwP2O9bdGMcYYyJkJYI9We0OWLkAls6FZS/BjnKQRBhxuCspjJoBA/eLdZTGmDjQUYnAEkFf0dgI6z5wJYWlL8DGxW5+3siWpDB8EiRaIdCYILJEEERbVrVUIZW+Do11kJYDBSXuNprDDoG9JkBmfqwjNcb0AmsjCKLcQph0uXvs2uZulbniZVj7gXutjW69nBEuITQlh6HF7vJVY0xgWCIIgrT+7n4IY0530zVVsP5jWPu+e6z7AJY865ZJgru15rAJLaWGwWPsqiRj+jBLBEGUmglFR7pHk6pylxCaksMXc+HDv7llSWkwZJxLDgNHwoB9IW9f6F8ACdYVxZg9nSUC42Tmw6jj3QNch7UtpV6J4UP3/MGjUOcb+ygx1fVnyNsXBuzjPXtJImsvSxLG7CGimghE5ATgLiAReFBVfxOy/ELgBm+yCrhCVT+OZkwmQiIwYG/3GHu2m9fYCNvXw+YvoeJL73mle17xCjTUtGyf1M/bPiRB5BZZkjAmzkQtEYhIIvBH4DigDHhPRJ5T1SW+1b4CpqjqFhGZAdwPTIpWTGY3JSRA9jD32Pvo1ssaG2Db2rYJonyp6+PQWNeybmKKa6TOKXSJIdd7bprul9OLH8oYE80SwURghaquBBCROcBpQHMiUNU3feu/DRREMR4TTQmJ3sl9hOsB7ddQD1vXwOaVrrqpcpV73rLKVTntqmy9flpO2+SQWwjZI9z7NDaANrjhuptfN4SZ39h6HVXILnClFOt9bUyzaCaCYcAa33QZHf/avxR4IdwCEbkMuAxgxIgRPRWf6S2JSS3VTOHsrGydHJqSxYbFrh9EQ23Px9Qvt6W6qvnZq8ZKy+759zMmjkUzEUiYeWF7r4nINFwimBxuuarej6s2oqSkZM/qAWc61y/HPYYWt13W1C6xpdRVPWkjJCS5y1wTklwJQRK91wm+14m+9bzX2giVa1q3cZS+AZ883vo90we2JIYB+0LePi3JIjWrVw6JMb0pmomgDBjumy4A1oWuJCLjgAeBGapaEcV4zJ7I3y7RE4aMbTuvbids/qptI/jKBfDx7Nbr9st11UvZI7xn75HjTWcMsoZws8eJZiJ4DxgpInsDa4HzgW/6VxCREcA/gG+r6rIoxmJM+5L7weAD3SNU7Y7WSWJrmXts+Qq+Wgi121uvn5DsJa7h3qMpUQyH/sNcQ3l3JSRa3w0TFVFLBKpaLyJXAi/hLh99SFUXi8hMb/l9wC1AHnCPiADUtzcWhjExkZIBQw5yj3B2bXWJoXKNaxBvShRb18BX/3XVWk3DefSEzMFef48ZsM9USEnvuX2bwLJB54yJpoY6lwwq17h7STTWd39f9Tth5X9dn43a7a7H9z5TYdQJ7tF/aE9FbfogG3TOmFhJTG65rLYnlHwX6mth1RvuiqplL7hRZgH2OrjlrnVDxrlOgcZEwEoExuzJVN09rJfOdQmhbBGgri1h1PHurnVFR0FyWu/HVl8D1RUtj51bID3P9QvpP8y1eZheY/cjMCYoqja6ntzLXnTDjddVQ3KG6+Q36njIHNL2Mtv2Lr+VRO/SW++1JEDNttYn9+oK2FHRdl715rYN6X4JSa4xPVzP8twid3WWlWh6lCUCY4Kobpe7smnZC7D0Rdje5urtnpGc4X7ppw/wnr1HRl7r6bRs2LGpbefBLaWwc3Prfab2dwnCnxxyCl1Saqh1j/paN76V/3W9tyzc68Y6SE53Pdf75bjntOyW1/286bSc3StBNTa69py6nS4R+5+T01uORy839FsiMCboVGHTMqjZ3nYojlbDdNR3PGRHWnbbE35yv92Pb9c2L0GsajsMSeUqqN/Vtf0lpkJSqmujSUyFpBR36W5ttRvSpLaq4+2T0tomh7RsdwzCneDrqlteRxprc1IIOZ7pA8PM89bbjfuCWGOxMUEnAvn7xzqK9qX1d539wnX4a2yEqg3uktzGBu+kHnqiT3Un+sQUN6+zaqWGOnfp785K97xri/e60jevsmXe9vVQ/oWr0kpOd8kvuZ+rwkru13peSkbbecnpLsba6rZVaNWb3OvNK910zbb24558LRw7a3eOdFiWCIwx8S0hwV0a25OXxyYmQ8ZA94g39bWuqqy6wlWl+ZNGQXS6WVkiMMaYeJKUAllD3KOXWF91Y4wJOEsExhgTcJYIjDEm4CwRGGNMwFkiMMaYgLNEYIwxAWeJwBhjAs76ERjTx9XWN1Kxo4aKqlqy0pIYmJlKRmp8/uurKhu317CqoprSih2srqhm1eZqVlXsoGzLTgRIT00kIyWJ9JRE0r3njNSmaTcvI7VlWct0Iom7eXe37H7JDMxMITM1CemlQfHqGxrZvKOW8qoasvslU5Db82MUxedfgzGmQ/6TQ/l299hUVes9e/Oq3OvK6ro226enJJKflcrAzFTyM1MZmJVCfmaa95zasiwrlbTknh0uur6hkXWVuyit2MGqzdWsrthBaUW1d9Lfwa66lju6JSYIw3L6UZiXzkHDshFgZ20DO2rrqa5tYEdNPZuqaqiubaC6tp4dNQ3srGvo0XjDSUtOaD4+Tc/uOKZ6x6/leKantD3NNjYqm6trm7+rluda3/fpnjdX19I0JNzMKfty44zRPf55LBEYEyc6Ozls8p30/ScHv4yUxOaT0chBmRy+T17zyWpARjJVNQ2t9rOpqoYvy6t456satoRJGABZqUnN+0hO6v6v4IZG5eutuyjbspP6xpbgU5MSKMxLZ8SADCaPHEhRXjoj8jIoHJDOsNx+JCd27Vd8Y6Oys85LFjXu2SWPBhobuz/IpqJs3VnXJvGu2VzNB6u2dPid5GelkpeZys7aBsqrati8o5aGMLGkJiW4pJKVyvAB6UwozPUlmhT2H9K/2/F3xBJBQFTX1vP5+u3kZ6ayV04aSV385+qqTVU1fL5+m/fYzpJ12/iyvKrVCaCrEgSGZrtfh4V5Ge55QMvreKzuUFUqq+ta/Upv9cuvqoZN3nNnJ4eBmeFPDu65/V+fkaqt90oZ22sor9rFpu2+EkdVDRVVNdTUdf/+yyIwZlg2J40bSuGADEbkpVOUl8GgrFQSEnqumiUhQchITXJ/D1k9tttONZXSNob7he8dv5x+yYwryG5TmhjofY+9WeXkF3//OaZHqCpL1m/jteWbWLisnEWlW6htcP/ESQlCQW6/5hPoiAHuH7IwL53hA9K7VBVQ39DIV5t2sKTphO+d/Mu31zSvM6R/GgcMzWLq/vmk7kY1g6tS2ElpRTUvLf6azTtqWy0fmJniPtOA9OaTzAgvWQzISOmxfzBVZXtNvftHb3Vyb1u8r9hRQ11D25N7cqI0VyUMzU5j7LBs3wkhLSYnh5SkBIZkpzEkOw3Ijvr79TVJiQkM6p/GoP4xuBvcbrJE0AW19Y2kJMXvhVabqmp4bXk5ry3bxMLlm9hU5U7Go4dkcfGRRRxSmEtldS2rfA1wH6zewvZdLTdUF3En7qbk0HRCLcxLZ1BWKl9t2sHn67c1n/iXbdhOTb1LMMmJwn6Dsjh6ZD4HDM3iwKH9GT20PwMyUqLyebftqnP1yl7d8mqvgfHtlRU889HaVsX0rNQkCgak79b319iozfXytfVtfxknJgh5GSnNv/T2H5LV8svdO7EP8pZl90uOyS8/Y8KxROCjqpRvr2HV5mpKN+1g9eZqrxHLNWpVVteRnpLYpjjnb2RraSzq+Ua2ULX1jSxatZmFyzbx2vJyFq9z45gPyEhh8n4DOXpUPkePHNjhL5SmqovSCu/zbmo5qb7yxcbmZBIqLyOFA4b25zuHF3LA0P4cMLQ/++Zn9mqi7J+WzEHDsjloWNtfr7vqGijb4iWJCpf01lbuDPvrPFIiMHJQZkiRvuVvITc9pUerOIzpLYG7Q1l9QyPrt3pXLHgniFUV1aze7E4Y/isO/FcsjBiQzpD+aa6xKKQqoKNGNn9iyMtMcXWXvsve0n3TTZe4packucvjUhNbNZSpKl9t2sHCZeUsXL6Jt1dWUF3bQFKCMKEwlymj8jl6ZD5j9urfYyekHTX13rHZwYZtNYzIS+fAof0ZlJVqv2iN2YPYHcqA+V9s5Of/Whz2ioURXoPjkfsNbGmI7MIVC3UNjVRU1ba6GiM0WXzx9TY2VdVSXVvfpV+lyYniJYZE6hvdNdYAhXnpnDWhgKNH5XPYPgPISuv+Lew6kpGa1PyL3xjTNwUmEQzISGHMsGxOHDu0pRExL53BWWm7/es5OdHfyNa52vpG37XQTddDe9dB1zZQXVPfcl20N72jtoGGRmXCiByOHpVPYV7GbsVsjDFNApMIiofn8MdvToh1GIC7OiMlKYHs9Oj8ijfGmK6I30tgjDHG9ApLBMYYE3CWCIwxJuAsERhjTMBZIjDGmICzRGCMMQFnicAYYwLOEoExxgTcHjfWkIiUA6vaWTwQ2NSL4XRVvMcH8R+jxbd7LL7dsyfHV6iq+eEW7HGJoCMisqi9QZXiQbzHB/Efo8W3eyy+3dNX47OqIWOMCThLBMYYE3B9LRHcH+sAOhHv8UH8x2jx7R6Lb/f0yfj6VBuBMcaYrutrJQJjjDFdZInAGGMCrs8kAhE5QUSWisgKEbkx1vGEEpFSEflURD4Ske7fdLnn4nlIRDaKyGe+eQNE5GURWe4958ZZfLNEZK13DD8SkRNjGN9wEZkvIp+LyGIRucabHxfHsIP44uIYikiaiLwrIh978f3cmx8vx6+9+OLi+PniTBSRD0XkeW+6W8evT7QRiEgisAw4DigD3gMuUNUlMQ3MR0RKgRJVjYvOKCJyNFAFPKqqB3nzbgM2q+pvvGSaq6o3xFF8s4AqVb09FjH5ichQYKiqfiAiWcD7wOnAxcTBMewgvnOJg2MoIgJkqGqViCQDrwPXAGcSH8evvfhOIA6OXxMRuQ4oAfqr6snd/R/uKyWCicAKVV2pqrXAHOC0GMcU11R1IbA5ZPZpwCPe60dwJ46YaCe+uKGq61X1A+/1duBzYBhxcgw7iC8uqFPlTSZ7DyV+jl978cUNESkATgIe9M3u1vHrK4lgGLDGN11GHP3RexT4j4i8LyKXxTqYdgxW1fXgTiTAoBjHE86VIvKJV3UUs6orPxEpAg4G3iEOj2FIfBAnx9Cr1vgI2Ai8rKpxdfzaiQ/i5PgBvwd+DDT65nXr+PWVRCBh5sVV9gaOVNUJwAzgh17Vh+mae4F9gfHAeuCO2IYDIpIJPA38SFW3xTqeUGHii5tjqKoNqjoeKAAmishBsYolnHbii4vjJyInAxtV9f2e2F9fSQRlwHDfdAGwLkaxhKWq67znjcAzuOqseLPBq1tuqmPeGON4WlHVDd4/ZyPwADE+hl7d8dPAY6r6D2923BzDcPHF2zH0YqoEFuDq3+Pm+DXxxxdHx+9I4FSv7XEOcIyI/I1uHr++kgjeA0aKyN4ikgKcDzwX45iaiUiG12CHiGQA3wA+63irmHgOuMh7fRHwzxjG0kbTH7jnDGJ4DL3GxD8Dn6vq//oWxcUxbC++eDmGIpIvIjne637AscAXxM/xCxtfvBw/Vf2JqhaoahHufPeqqn6L7h4/Ve0TD+BE3JVDXwI3xTqekNj2AT72HovjIT5gNq5oW4crUV0K5AGvAMu95wFxFt9fgU+BT7w/+KExjG8yrvrxE+Aj73FivBzDDuKLi2MIjAM+9OL4DLjFmx8vx6+9+OLi+IXEOhV4fneOX5+4fNQYY0z39ZWqIWOMMd1kicAYYwLOEoExxgScJQJjjAk4SwTGGBNwlghMTImIisgdvunrvcHlemLfD4vI2T2xr07e5xxvlM/5PbCvX4jIsZ2sM0tErg8zv0h8o7UaEylLBCbWaoAzRWRgrAPx80a0jdSlwA9Uddruvq+q3qKq83Z3P93Rxc9s+hBLBCbW6nH3Wb02dEHoL3oRqfKep4rIf0XkCRFZJiK/EZELvfHjPxWRfX27OVZEXvPWO9nbPlFEfici73mDh13u2+98Efk7rtNQaDwXePv/TER+6827Bdd56z4R+V3I+lNFZIGIPCUiX4jIY16PX0TkEO8zvC8iL/mGBWj+zCJyorfd6yJyt3hjznsO9Pa9UkSu9s1PEpFHvM/1lIike/uaLm7c+k+9wdJSvfmlInKLiLwOnCMiV4vIEm/7ORF8f6YviHWvOHsE+4G750B/oBTIBq4HZnnLHgbO9q/rPU8FKoGhQCqwFvi5t+wa4Pe+7V/E/eAZieuhnAZcBtzsrZMKLAL29va7A9g7TJx7AauBfCAJeBU43Vu2AHevidBtpgJbcWNfJQBv4ZJGMvAmkO+tdx7wkP8ze3GuaYoF19O6qffoLG/7VGAgUOHtswjXm/hIb72HvOPZtK9R3vxHcYPQ4R33H/tiXgekeq9zYv33YY/eeViJwMSculExHwWu7mxdn/fUjblfgxtW5D/e/E9xJ8QmT6hqo6ouB1YCo3FjPX1H3BDD7+C65Y/01n9XVb8K836HAgtUtVxV64HHgEhGkH1XVcvUDVL2kRfb/sBBwMteDDfjkoXfaGClL5bZIcv/rao16m50tBEY7M1fo6pveK//hks8+wNfqeoyb/4jIbE/7nv9CfCYiHwLV1ozAZAU6wCM8fwe+AD4i29ePV71pVelkuJbVuN73eibbqT133XoGCqKG7b8KlV9yb9ARKbiSgThhBvqPBL+OBu82ARYrKqHd7BdZ+8Xbr/Q/uftiP8zn4RLEqcCPxORMV7iM32YlQhMXFDVzcATuIbXJqXAId7r03DVH111jogkeO0G+wBLgZeAK7xhmhGRUd6osB15B5giIgO9RtULgP92Ix68GPJF5HDv/ZNFZEzIOl8A+4i7qQy46qNIjGjarxfj696+ikRkP2/+t8PFLiIJwHBVnY+74UkOkBnh+5o9mJUITDy5A7jSN/0A8E8ReRc3kmJ7v9Y7shR30hsMzFTVXSLyIK6K5gOvpFFOJ7f0U9X1IvITYD7uF/ZcVe3WEMmqWus1CN8tItm4/8Pf40ambVpnp4j8AHhRRDYB70a4+8+Bi0TkT7gRKO/1PvMlwJMikoQbtv2+MNsmAn/zYhLgTnVj8Zs+zkYfNSZOiUimupunC/BHYLmq3hnruEzfY1VDxsSv73uNyYtxV1T9KcbxmD7KSgTGGBNwViIwxpiAs0RgjDEBZ4nAGGMCzhKBMcYEnCUCY4wJuP8fJenZARxUmKAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate plot\n",
    "plt.title('k-NN Varying number of neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy of 9.8% with the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09824561403508772"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#which iteration is the best:\n",
    "(list(test_accuracy).index(max(test_accuracy))*2)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() missing 1 required positional argument: 'base_estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='classifier')\n",
    "\n",
    "all_clfs = []\n",
    "for name, ClassifierClass in estimators:\n",
    "    #print('Appending', name)\n",
    "    try:\n",
    "        clf = ClassifierClass()\n",
    "        all_clfs.append(clf)\n",
    "    except Exception as e:\n",
    "        #print('Unable to import', name)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07017543859649122\n",
      "0.07368421052631578\n",
      "0.08070175438596491\n",
      "0.0912280701754386\n",
      "Negative values in data passed to CategoricalNB (input X)\n",
      "Negative values in data passed to ComplementNB (input X)\n",
      "0.07017543859649122\n",
      "0.07368421052631578\n",
      "0.05263157894736842\n",
      "0.056140350877192984\n",
      "0.07368421052631578\n",
      "0.08421052631578947\n",
      "0.05263157894736842\n",
      "0.07017543859649122\n",
      "0.06315789473684211\n",
      "0.05964912280701754\n",
      "0.06315789473684211\n",
      "0.04912280701754386\n",
      "0.08421052631578947\n",
      "0.09824561403508772\n",
      "0.08771929824561403\n",
      "0.09473684210526316\n",
      "Negative values in data passed to MultinomialNB (input X)\n",
      "0.08070175438596491\n",
      "specified nu is infeasible\n",
      "0.08070175438596491\n",
      "0.05263157894736842\n",
      "0.05263157894736842\n",
      "No neighbors found for test samples array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "        67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
      "        80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  92,  93,\n",
      "        94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106,\n",
      "       107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
      "       120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n",
      "       133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145,\n",
      "       146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
      "       159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
      "       172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
      "       185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "       198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "       212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
      "       264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276,\n",
      "       277, 278, 279, 280, 281, 282, 284], dtype=int64), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n",
      "0.07368421052631578\n",
      "0.0912280701754386\n",
      "0.08771929824561403\n",
      "0.09824561403508772\n",
      "0.07719298245614035\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for algo in all_clfs:\n",
    "    try:\n",
    "        algo.fit(X_train, y_train)\n",
    "        performance = algo.score(X_test, y_test)\n",
    "        results.append(performance)\n",
    "        print(performance)\n",
    "    except Exception as e:\n",
    "        #print('Unable to import', name)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09824561403508772"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0912280701754386"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "mod = SGDClassifier()\n",
    "mod.fit(X_train, y_train)\n",
    "mod.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a 33 of MSE with the best model availble for classification, not any better than with the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.256140350877196"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Regressor (as baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we look at the regression methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001385750811327524"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "dummy_regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.256140350877196"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() missing 1 required positional argument: 'estimator'\n",
      "__init__() missing 1 required positional argument: 'base_estimator'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n",
      "__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='regressor')\n",
    "\n",
    "all_regs = []\n",
    "for name, ClassifierClass in estimators:\n",
    "    #print('Appending', name)\n",
    "    try:\n",
    "        clf = ClassifierClass()\n",
    "        all_regs.append(clf)\n",
    "    except Exception as e:\n",
    "        #print('Unable to import', name)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to stop the cell from running cause it was way too long, don't really know why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1794208786297632\n",
      "0.17291238758658956\n",
      "0.12192083306015922\n",
      "0.2070590333735608\n",
      "0.004455394025421278\n",
      "-0.752968121497742\n",
      "-0.001385750811327524\n",
      "-0.001385750811327524\n",
      "0.20548353050792356\n",
      "-0.8858249420548638\n",
      "0.22279000407833427\n",
      "0.12663618068533244\n",
      "-38231.551483496696\n",
      "0.15576724809857678\n",
      "0.0976254204373056\n",
      "0.11367093167210507\n",
      "Isotonic regression input X should be a 1d array or 2d array with 1 feature\n",
      "0.015447151336110854\n",
      "-2604.597014422334\n",
      "-5560753.258073808\n",
      "0.1289904534227041\n",
      "-0.001385750811327524\n",
      "0.2079607623073092\n",
      "-0.001385750811327524\n",
      "0.19374231835660893\n",
      "0.19383250400952723\n",
      "-2.2129588357310146\n",
      "-8797.369557183014\n",
      "-22346.037135014925\n",
      "For mono-task outputs, use ElasticNet\n",
      "For mono-task outputs, use ElasticNetCVCV\n",
      "For mono-task outputs, use ElasticNet\n",
      "For mono-task outputs, use LassoCVCV\n",
      "0.2367780465126712\n",
      "0.13092302091289765\n",
      "0.16887027863540638\n",
      "-29.099456838501446\n",
      "0.20005668466530335\n",
      "-0.568533045497823\n",
      "0.17662194370935225\n",
      "-204.3455302126896\n",
      "Input contains NaN, infinity or a value too large for dtype('float64').\n",
      "0.19069514856515857\n",
      "0.18338857482058324\n",
      "0.19784472931674535\n",
      "-18.88900274470269\n",
      "0.2497156623604575\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-0e8085bd73bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malgo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_regs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mperformance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperformance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_theil_sen.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    385\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_lstsq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             for job in range(n_jobs))\n\u001b[0m\u001b[0;32m    388\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         self.n_iter_, coefs = _spatial_median(weights,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_theil_sen.py\u001b[0m in \u001b[0;36m_lstsq\u001b[1;34m(X, y, indices, fit_intercept)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mX_subpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0my_subpopulation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_subsamples\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         weights[index] = lstsq(X_subpopulation,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_regs = []\n",
    "for algo in all_regs:\n",
    "    try:\n",
    "        algo.fit(X_train, y_train)\n",
    "        performance = algo.score(X_test, y_test)\n",
    "        results.append(performance)\n",
    "        print(performance)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16051294344183276"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg = GradientBoostingRegressor(random_state=1)\n",
    "reg.fit(X_train, y_train)\n",
    "GradientBoostingRegressor(random_state=0)\n",
    "y_pred = reg.predict(X_test)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.439357370697504"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 75 candidates, totalling 225 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(random_state=1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.025, 0.05, 0.075, 0.1],\n",
       "                         'max_depth': [4, 5, 6], 'max_features': [1.0],\n",
       "                         'min_samples_leaf': [3],\n",
       "                         'n_estimators': [100, 200, 300, 400, 500]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid={'n_estimators':[100,200,300,400,500], \n",
    "            'learning_rate': [0.1,0.025,0.05,0.075,0.1],\n",
    "            'max_depth':[4,5,6], \n",
    "            'min_samples_leaf':[3], \n",
    "            'max_features':[1.0] } \n",
    "\n",
    "reg = GradientBoostingRegressor(random_state=1)\n",
    "grid_search = GridSearchCV(estimator = reg, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19936215384251366"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "grid_search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.44720138825216"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0976254204373056"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "est = HistGradientBoostingRegressor().fit(X_train, y_train)\n",
    "est.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=HistGradientBoostingRegressor(random_state=1),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.05, 0.1], 'max_iter': [100, 200],\n",
       "                         'max_leaf_nodes': [10, 20, 40, 50],\n",
       "                         'min_samples_leaf': [10, 30]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "param_grid={'learning_rate': [0.05,0.1],\n",
    "            'max_iter':[100,200], \n",
    "            'max_leaf_nodes':[10,20,40,50], \n",
    "            'min_samples_leaf':[10,30] } \n",
    "\n",
    "reg = HistGradientBoostingRegressor(random_state=1)\n",
    "grid_search = GridSearchCV(estimator = reg, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.04541915487773"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = est.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.14469607945136"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regr = MLPRegressor(hidden_layer_sizes=500, random_state=1, max_iter=20000).fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_test)\n",
    "regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tfidf_matrix.toarray()\n",
    "a = a.tolist()\n",
    "\n",
    "X_train = a\n",
    "y_train = list(df_fr['date'])\n",
    "\n",
    "a = []\n",
    "for item in y_train:\n",
    "    a.append(float(item))\n",
    "    \n",
    "y_train = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = numpy.array(X_train)\n",
    "y_train = numpy.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 849)               721650    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2670)              2269500   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2670)              7131570   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 2671      \n",
      "=================================================================\n",
      "Total params: 10,125,391\n",
      "Trainable params: 10,125,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(2670, activation='relu'))\n",
    "model.add(Dense(2670, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that our model performed quite poorly, and it could be for many reasons. One of the main reasons we could think of is that we have too few data (indeed 1000 is extremely low to train deep learning models). This dataset being very small we can imagine that there is too few example of each kind for the model to learn how to predict them properly (what caracterizes them), no matter how much epochs we perform, since the problem comes from the relationship between dimensions that cannot be discovered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "13/13 [==============================] - 1s 109ms/step - loss: 3313927.7500 - mse: 3313928.0000 - mae: 1799.7444 - val_loss: 651141.5000 - val_mse: 651141.4375 - val_mae: 784.6620\n",
      "Epoch 2/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 397786.7188 - mse: 397786.7188 - mae: 519.4692 - val_loss: 415035.6875 - val_mse: 415035.6875 - val_mae: 611.2809\n",
      "Epoch 3/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 218622.5781 - mse: 218622.5781 - mae: 395.8951 - val_loss: 112576.6016 - val_mse: 112576.6016 - val_mae: 274.9587\n",
      "Epoch 4/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 108639.6328 - mse: 108639.6328 - mae: 269.6002 - val_loss: 82123.4297 - val_mse: 82123.4297 - val_mae: 233.5014\n",
      "Epoch 5/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 79602.3359 - mse: 79602.3359 - mae: 224.9866 - val_loss: 69843.0625 - val_mse: 69843.0625 - val_mae: 210.3862\n",
      "Epoch 6/150\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 58710.8008 - mse: 58710.8008 - mae: 193.7036 - val_loss: 61895.4531 - val_mse: 61895.4531 - val_mae: 197.6021\n",
      "Epoch 7/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 50696.3516 - mse: 50696.3516 - mae: 180.0389 - val_loss: 56505.2461 - val_mse: 56505.2461 - val_mae: 189.7470\n",
      "Epoch 8/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 44859.3125 - mse: 44859.3125 - mae: 169.1075 - val_loss: 54356.3711 - val_mse: 54356.3711 - val_mae: 187.7800\n",
      "Epoch 9/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 38390.1875 - mse: 38390.1875 - mae: 155.9213 - val_loss: 51073.7812 - val_mse: 51073.7812 - val_mae: 181.3023\n",
      "Epoch 10/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 33563.7539 - mse: 33563.7539 - mae: 146.1482 - val_loss: 48779.6445 - val_mse: 48779.6445 - val_mae: 177.6346\n",
      "Epoch 11/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 29783.8359 - mse: 29783.8320 - mae: 137.9577 - val_loss: 47046.0625 - val_mse: 47046.0625 - val_mae: 173.4772\n",
      "Epoch 12/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 26092.6484 - mse: 26092.6484 - mae: 128.8117 - val_loss: 44587.5078 - val_mse: 44587.5078 - val_mae: 170.3335\n",
      "Epoch 13/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 22990.9961 - mse: 22990.9941 - mae: 121.2686 - val_loss: 42724.0195 - val_mse: 42724.0195 - val_mae: 167.0786\n",
      "Epoch 14/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 20305.6543 - mse: 20305.6543 - mae: 114.1394 - val_loss: 41604.8867 - val_mse: 41604.8867 - val_mae: 163.1785\n",
      "Epoch 15/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 18133.8125 - mse: 18133.8125 - mae: 107.9442 - val_loss: 40185.4922 - val_mse: 40185.4922 - val_mae: 160.2290\n",
      "Epoch 16/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 15850.5938 - mse: 15850.5928 - mae: 100.8221 - val_loss: 38509.3125 - val_mse: 38509.3125 - val_mae: 156.6433\n",
      "Epoch 17/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 14048.9365 - mse: 14048.9365 - mae: 95.1789 - val_loss: 36749.1055 - val_mse: 36749.1055 - val_mae: 152.4559\n",
      "Epoch 18/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 12879.0430 - mse: 12879.0430 - mae: 90.9942 - val_loss: 37060.3359 - val_mse: 37060.3359 - val_mae: 150.7241\n",
      "Epoch 19/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 11236.9365 - mse: 11236.9355 - mae: 85.5113 - val_loss: 34105.6719 - val_mse: 34105.6719 - val_mae: 147.0800\n",
      "Epoch 20/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 9502.2021 - mse: 9502.2021 - mae: 78.6492 - val_loss: 33340.6523 - val_mse: 33340.6523 - val_mae: 143.3212\n",
      "Epoch 21/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 8384.7002 - mse: 8384.7002 - mae: 73.7490 - val_loss: 32118.6230 - val_mse: 32118.6230 - val_mae: 140.6951\n",
      "Epoch 22/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 7305.4492 - mse: 7305.4492 - mae: 68.8081 - val_loss: 30892.9375 - val_mse: 30892.9375 - val_mae: 138.4010\n",
      "Epoch 23/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 6510.6250 - mse: 6510.6250 - mae: 64.6559 - val_loss: 30253.6621 - val_mse: 30253.6621 - val_mae: 135.3411\n",
      "Epoch 24/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 5661.3433 - mse: 5661.3433 - mae: 60.4963 - val_loss: 29217.3066 - val_mse: 29217.3066 - val_mae: 133.1391\n",
      "Epoch 25/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 4961.4380 - mse: 4961.4380 - mae: 56.9826 - val_loss: 28269.0742 - val_mse: 28269.0742 - val_mae: 130.8127\n",
      "Epoch 26/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 4350.8135 - mse: 4350.8135 - mae: 52.8419 - val_loss: 27656.8633 - val_mse: 27656.8633 - val_mae: 128.6083\n",
      "Epoch 27/150\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 3839.2942 - mse: 3839.2942 - mae: 49.1501 - val_loss: 26502.3984 - val_mse: 26502.3984 - val_mae: 127.0849\n",
      "Epoch 28/150\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 3283.6140 - mse: 3283.6140 - mae: 45.6512 - val_loss: 25903.6289 - val_mse: 25903.6289 - val_mae: 125.0195\n",
      "Epoch 29/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 2957.9321 - mse: 2957.9321 - mae: 42.9243 - val_loss: 25539.6738 - val_mse: 25539.6738 - val_mae: 123.2760\n",
      "Epoch 30/150\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 2513.5938 - mse: 2513.5938 - mae: 39.7295 - val_loss: 24303.5898 - val_mse: 24303.5898 - val_mae: 121.3924\n",
      "Epoch 31/150\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 2256.6067 - mse: 2256.6067 - mae: 37.8602 - val_loss: 24031.9844 - val_mse: 24031.9844 - val_mae: 119.7359\n",
      "Epoch 32/150\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 1861.9742 - mse: 1861.9742 - mae: 34.2037 - val_loss: 23053.5449 - val_mse: 23053.5449 - val_mae: 117.9135\n",
      "Epoch 33/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 1605.8467 - mse: 1605.8467 - mae: 31.5894 - val_loss: 22693.5078 - val_mse: 22693.5078 - val_mae: 116.4376\n",
      "Epoch 34/150\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 1419.8848 - mse: 1419.8848 - mae: 29.8769 - val_loss: 21897.6289 - val_mse: 21897.6289 - val_mae: 115.2906\n",
      "Epoch 35/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 1258.3380 - mse: 1258.3380 - mae: 27.5724 - val_loss: 21688.1426 - val_mse: 21688.1426 - val_mae: 113.6274\n",
      "Epoch 36/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 1015.6825 - mse: 1015.6825 - mae: 24.8446 - val_loss: 21033.4824 - val_mse: 21033.4824 - val_mae: 112.3322\n",
      "Epoch 37/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 884.1975 - mse: 884.1975 - mae: 23.4046 - val_loss: 20718.5879 - val_mse: 20718.5898 - val_mae: 111.2866\n",
      "Epoch 38/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 778.4218 - mse: 778.4218 - mae: 21.9827 - val_loss: 20445.8496 - val_mse: 20445.8496 - val_mae: 110.3302\n",
      "Epoch 39/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 703.8311 - mse: 703.8311 - mae: 20.7683 - val_loss: 19753.1816 - val_mse: 19753.1816 - val_mae: 109.6193\n",
      "Epoch 40/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 589.5242 - mse: 589.5242 - mae: 18.9932 - val_loss: 19492.8438 - val_mse: 19492.8438 - val_mae: 108.3720\n",
      "Epoch 41/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 489.8116 - mse: 489.8116 - mae: 17.1047 - val_loss: 19284.2441 - val_mse: 19284.2441 - val_mae: 107.4248\n",
      "Epoch 42/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 400.8923 - mse: 400.8923 - mae: 15.6304 - val_loss: 18876.0391 - val_mse: 18876.0391 - val_mae: 106.4882\n",
      "Epoch 43/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 343.5853 - mse: 343.5853 - mae: 14.3747 - val_loss: 18639.4707 - val_mse: 18639.4707 - val_mae: 105.7751\n",
      "Epoch 44/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 303.7808 - mse: 303.7808 - mae: 13.5691 - val_loss: 18252.5605 - val_mse: 18252.5605 - val_mae: 104.9255\n",
      "Epoch 45/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 265.9125 - mse: 265.9125 - mae: 12.6073 - val_loss: 18247.7793 - val_mse: 18247.7793 - val_mae: 104.4955\n",
      "Epoch 46/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 221.1949 - mse: 221.1949 - mae: 11.4206 - val_loss: 17858.2500 - val_mse: 17858.2500 - val_mae: 103.4854\n",
      "Epoch 47/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 191.9935 - mse: 191.9935 - mae: 10.6898 - val_loss: 17808.1328 - val_mse: 17808.1328 - val_mae: 103.3868\n",
      "Epoch 48/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 158.1711 - mse: 158.1711 - mae: 9.6084 - val_loss: 17640.5215 - val_mse: 17640.5215 - val_mae: 102.8736\n",
      "Epoch 49/150\n",
      "13/13 [==============================] - 1s 88ms/step - loss: 138.7460 - mse: 138.7460 - mae: 9.0165 - val_loss: 17423.6797 - val_mse: 17423.6797 - val_mae: 102.2132\n",
      "Epoch 50/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 117.8196 - mse: 117.8196 - mae: 8.1815 - val_loss: 17300.5371 - val_mse: 17300.5371 - val_mae: 101.7208\n",
      "Epoch 51/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 106.5881 - mse: 106.5881 - mae: 7.8970 - val_loss: 17205.1504 - val_mse: 17205.1504 - val_mae: 101.4038\n",
      "Epoch 52/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 101.1407 - mse: 101.1407 - mae: 7.8907 - val_loss: 17040.9121 - val_mse: 17040.9121 - val_mae: 101.0452\n",
      "Epoch 53/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 80.7349 - mse: 80.7349 - mae: 6.8226 - val_loss: 16894.2617 - val_mse: 16894.2617 - val_mae: 100.6844\n",
      "Epoch 54/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 69.9715 - mse: 69.9715 - mae: 6.3008 - val_loss: 16792.2676 - val_mse: 16792.2676 - val_mae: 100.3200\n",
      "Epoch 55/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 56.2195 - mse: 56.2195 - mae: 5.6323 - val_loss: 16684.6914 - val_mse: 16684.6914 - val_mae: 100.0960\n",
      "Epoch 56/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 47.4440 - mse: 47.4440 - mae: 5.1025 - val_loss: 16563.1133 - val_mse: 16563.1133 - val_mae: 99.6957\n",
      "Epoch 57/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 40.5871 - mse: 40.5871 - mae: 4.6665 - val_loss: 16529.8496 - val_mse: 16529.8496 - val_mae: 99.5696\n",
      "Epoch 58/150\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 36.0207 - mse: 36.0207 - mae: 4.3701 - val_loss: 16426.4590 - val_mse: 16426.4590 - val_mae: 99.3373\n",
      "Epoch 59/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 32.3995 - mse: 32.3995 - mae: 4.1743 - val_loss: 16358.2178 - val_mse: 16358.2178 - val_mae: 99.0477\n",
      "Epoch 60/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 29.5930 - mse: 29.5930 - mae: 3.9504 - val_loss: 16335.4609 - val_mse: 16335.4609 - val_mae: 99.0919\n",
      "Epoch 61/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 26.2699 - mse: 26.2699 - mae: 3.8390 - val_loss: 16261.2520 - val_mse: 16261.2510 - val_mae: 98.8135\n",
      "Epoch 62/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 21.5280 - mse: 21.5280 - mae: 3.3324 - val_loss: 16243.1338 - val_mse: 16243.1338 - val_mae: 98.7089\n",
      "Epoch 63/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 18.6906 - mse: 18.6906 - mae: 3.1031 - val_loss: 16216.9688 - val_mse: 16216.9688 - val_mae: 98.5785\n",
      "Epoch 64/150\n",
      "13/13 [==============================] - 1s 105ms/step - loss: 16.7634 - mse: 16.7634 - mae: 2.9303 - val_loss: 16163.2480 - val_mse: 16163.2480 - val_mae: 98.4462\n",
      "Epoch 65/150\n",
      "13/13 [==============================] - 1s 102ms/step - loss: 15.3148 - mse: 15.3148 - mae: 2.7763 - val_loss: 16140.6914 - val_mse: 16140.6914 - val_mae: 98.3895\n",
      "Epoch 66/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 14.1665 - mse: 14.1665 - mae: 2.7318 - val_loss: 16052.6602 - val_mse: 16052.6602 - val_mae: 98.2072\n",
      "Epoch 67/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 11.0328 - mse: 11.0328 - mae: 2.3470 - val_loss: 16035.1055 - val_mse: 16035.1055 - val_mae: 98.1281\n",
      "Epoch 68/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 9.5493 - mse: 9.5493 - mae: 2.1588 - val_loss: 16037.2129 - val_mse: 16037.2129 - val_mae: 98.0662\n",
      "Epoch 69/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 8.6554 - mse: 8.6554 - mae: 2.0359 - val_loss: 16032.3711 - val_mse: 16032.3711 - val_mae: 98.0411\n",
      "Epoch 70/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 7.8051 - mse: 7.8051 - mae: 1.9276 - val_loss: 16023.9785 - val_mse: 16023.9785 - val_mae: 97.9813\n",
      "Epoch 71/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 9.1062 - mse: 9.1062 - mae: 2.2383 - val_loss: 15955.3145 - val_mse: 15955.3145 - val_mae: 97.8850\n",
      "Epoch 72/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 8.5879 - mse: 8.5879 - mae: 2.1844 - val_loss: 15912.9863 - val_mse: 15912.9863 - val_mae: 97.7742\n",
      "Epoch 73/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 6.2722 - mse: 6.2722 - mae: 1.7546 - val_loss: 15935.5811 - val_mse: 15935.5811 - val_mae: 97.7767\n",
      "Epoch 74/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 5.0008 - mse: 5.0008 - mae: 1.5485 - val_loss: 15955.4375 - val_mse: 15955.4375 - val_mae: 97.7796\n",
      "Epoch 75/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 4.8537 - mse: 4.8537 - mae: 1.5721 - val_loss: 15887.7627 - val_mse: 15887.7627 - val_mae: 97.6329\n",
      "Epoch 76/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 3.9454 - mse: 3.9454 - mae: 1.3304 - val_loss: 15887.0576 - val_mse: 15887.0576 - val_mae: 97.6367\n",
      "Epoch 77/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 3.2620 - mse: 3.2620 - mae: 1.1824 - val_loss: 15878.8135 - val_mse: 15878.8135 - val_mae: 97.6090\n",
      "Epoch 78/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 2.7844 - mse: 2.7844 - mae: 1.0928 - val_loss: 15863.6846 - val_mse: 15863.6846 - val_mae: 97.5703\n",
      "Epoch 79/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 2.5146 - mse: 2.5146 - mae: 0.9860 - val_loss: 15862.9092 - val_mse: 15862.9092 - val_mae: 97.5346\n",
      "Epoch 80/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 2.2101 - mse: 2.2101 - mae: 0.9292 - val_loss: 15861.6768 - val_mse: 15861.6758 - val_mae: 97.5478\n",
      "Epoch 81/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 1.9126 - mse: 1.9126 - mae: 0.8396 - val_loss: 15849.8330 - val_mse: 15849.8340 - val_mae: 97.5078\n",
      "Epoch 82/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 1.7036 - mse: 1.7036 - mae: 0.7793 - val_loss: 15837.1436 - val_mse: 15837.1436 - val_mae: 97.4602\n",
      "Epoch 83/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 1.5245 - mse: 1.5245 - mae: 0.7458 - val_loss: 15832.2520 - val_mse: 15832.2520 - val_mae: 97.4787\n",
      "Epoch 84/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 1.2606 - mse: 1.2606 - mae: 0.6576 - val_loss: 15821.0732 - val_mse: 15821.0732 - val_mae: 97.4379\n",
      "Epoch 85/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 1.2798 - mse: 1.2798 - mae: 0.6822 - val_loss: 15824.9033 - val_mse: 15824.9033 - val_mae: 97.4396\n",
      "Epoch 86/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 1.2091 - mse: 1.2091 - mae: 0.6711 - val_loss: 15813.1865 - val_mse: 15813.1865 - val_mae: 97.3931\n",
      "Epoch 87/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 0.9903 - mse: 0.9903 - mae: 0.6041 - val_loss: 15811.9834 - val_mse: 15811.9834 - val_mae: 97.3943\n",
      "Epoch 88/150\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 0.8807 - mse: 0.8807 - mae: 0.5522 - val_loss: 15795.6016 - val_mse: 15795.6016 - val_mae: 97.3885\n",
      "Epoch 89/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.7951 - mse: 0.7951 - mae: 0.5317 - val_loss: 15805.5049 - val_mse: 15805.5039 - val_mae: 97.3732\n",
      "Epoch 90/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 0.7311 - mse: 0.7311 - mae: 0.4937 - val_loss: 15794.4404 - val_mse: 15794.4404 - val_mae: 97.3570\n",
      "Epoch 91/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.6767 - mse: 0.6767 - mae: 0.4494 - val_loss: 15790.8701 - val_mse: 15790.8701 - val_mae: 97.3480\n",
      "Epoch 92/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 0.6349 - mse: 0.6349 - mae: 0.4237 - val_loss: 15782.6182 - val_mse: 15782.6182 - val_mae: 97.3386\n",
      "Epoch 93/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.5475 - mse: 0.5475 - mae: 0.4324 - val_loss: 15784.8447 - val_mse: 15784.8447 - val_mae: 97.3195\n",
      "Epoch 94/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.4740 - mse: 0.4740 - mae: 0.3898 - val_loss: 15780.9033 - val_mse: 15780.9033 - val_mae: 97.3225\n",
      "Epoch 95/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.4506 - mse: 0.4506 - mae: 0.3714 - val_loss: 15768.0410 - val_mse: 15768.0410 - val_mae: 97.2925\n",
      "Epoch 96/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.4075 - mse: 0.4075 - mae: 0.3734 - val_loss: 15780.7178 - val_mse: 15780.7178 - val_mae: 97.3036\n",
      "Epoch 97/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.3819 - mse: 0.3819 - mae: 0.3629 - val_loss: 15766.8701 - val_mse: 15766.8701 - val_mae: 97.2855\n",
      "Epoch 98/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.3882 - mse: 0.3882 - mae: 0.3933 - val_loss: 15772.6738 - val_mse: 15772.6738 - val_mae: 97.2891\n",
      "Epoch 99/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.2921 - mse: 0.2921 - mae: 0.3130 - val_loss: 15770.7207 - val_mse: 15770.7207 - val_mae: 97.2891\n",
      "Epoch 100/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.2535 - mse: 0.2535 - mae: 0.2807 - val_loss: 15763.8340 - val_mse: 15763.8340 - val_mae: 97.2775\n",
      "Epoch 101/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.2295 - mse: 0.2295 - mae: 0.2701 - val_loss: 15766.2148 - val_mse: 15766.2148 - val_mae: 97.2785\n",
      "Epoch 102/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.1859 - mse: 0.1859 - mae: 0.2182 - val_loss: 15766.7354 - val_mse: 15766.7354 - val_mae: 97.2790\n",
      "Epoch 103/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.2145 - mse: 0.2145 - mae: 0.2517 - val_loss: 15755.4492 - val_mse: 15755.4492 - val_mae: 97.2536\n",
      "Epoch 104/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.2105 - mse: 0.2105 - mae: 0.2599 - val_loss: 15759.3242 - val_mse: 15759.3242 - val_mae: 97.2574\n",
      "Epoch 105/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.1683 - mse: 0.1683 - mae: 0.2333 - val_loss: 15760.6875 - val_mse: 15760.6875 - val_mae: 97.2627\n",
      "Epoch 106/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.1752 - mse: 0.1752 - mae: 0.2155 - val_loss: 15751.3652 - val_mse: 15751.3662 - val_mae: 97.2337\n",
      "Epoch 107/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.1443 - mse: 0.1443 - mae: 0.2067 - val_loss: 15756.5381 - val_mse: 15756.5381 - val_mae: 97.2503\n",
      "Epoch 108/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.1203 - mse: 0.1203 - mae: 0.1823 - val_loss: 15753.8154 - val_mse: 15753.8154 - val_mae: 97.2382\n",
      "Epoch 109/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.1215 - mse: 0.1215 - mae: 0.1979 - val_loss: 15759.7549 - val_mse: 15759.7549 - val_mae: 97.2490\n",
      "Epoch 110/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.1127 - mse: 0.1127 - mae: 0.2052 - val_loss: 15751.4336 - val_mse: 15751.4336 - val_mae: 97.2264\n",
      "Epoch 111/150\n",
      "13/13 [==============================] - 1s 91ms/step - loss: 0.0850 - mse: 0.0850 - mae: 0.1649 - val_loss: 15749.2090 - val_mse: 15749.2090 - val_mae: 97.2335\n",
      "Epoch 112/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.0836 - mse: 0.0836 - mae: 0.1673 - val_loss: 15745.8291 - val_mse: 15745.8291 - val_mae: 97.2289\n",
      "Epoch 113/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.0771 - mse: 0.0771 - mae: 0.1517 - val_loss: 15750.4209 - val_mse: 15750.4209 - val_mae: 97.2319\n",
      "Epoch 114/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 0.0704 - mse: 0.0704 - mae: 0.1416 - val_loss: 15752.6738 - val_mse: 15752.6738 - val_mae: 97.2250\n",
      "Epoch 115/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.0704 - mse: 0.0704 - mae: 0.1559 - val_loss: 15747.5400 - val_mse: 15747.5400 - val_mae: 97.2292\n",
      "Epoch 116/150\n",
      "13/13 [==============================] - 1s 99ms/step - loss: 0.0511 - mse: 0.0511 - mae: 0.1248 - val_loss: 15747.0811 - val_mse: 15747.0811 - val_mae: 97.2229\n",
      "Epoch 117/150\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 0.0446 - mse: 0.0446 - mae: 0.1116 - val_loss: 15746.8857 - val_mse: 15746.8857 - val_mae: 97.2252\n",
      "Epoch 118/150\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 0.0377 - mse: 0.0377 - mae: 0.0949 - val_loss: 15746.1943 - val_mse: 15746.1943 - val_mae: 97.2179\n",
      "Epoch 119/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 0.0331 - mse: 0.0331 - mae: 0.0906 - val_loss: 15746.9922 - val_mse: 15746.9922 - val_mae: 97.2205\n",
      "Epoch 120/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.0350 - mse: 0.0350 - mae: 0.0891 - val_loss: 15744.2285 - val_mse: 15744.2285 - val_mae: 97.2157\n",
      "Epoch 121/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0272 - mse: 0.0272 - mae: 0.0827 - val_loss: 15745.0869 - val_mse: 15745.0869 - val_mae: 97.2181\n",
      "Epoch 122/150\n",
      "13/13 [==============================] - 1s 96ms/step - loss: 0.0266 - mse: 0.0266 - mae: 0.0882 - val_loss: 15743.0010 - val_mse: 15743.0010 - val_mae: 97.2122\n",
      "Epoch 123/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.0275 - mse: 0.0275 - mae: 0.0955 - val_loss: 15741.8145 - val_mse: 15741.8145 - val_mae: 97.2142\n",
      "Epoch 124/150\n",
      "13/13 [==============================] - 1s 93ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.0844 - val_loss: 15741.1992 - val_mse: 15741.1992 - val_mae: 97.2098\n",
      "Epoch 125/150\n",
      "13/13 [==============================] - 1s 100ms/step - loss: 0.0252 - mse: 0.0252 - mae: 0.0882 - val_loss: 15742.2305 - val_mse: 15742.2305 - val_mae: 97.2138\n",
      "Epoch 126/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.0745 - val_loss: 15741.6475 - val_mse: 15741.6475 - val_mae: 97.2106\n",
      "Epoch 127/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0160 - mse: 0.0160 - mae: 0.0676 - val_loss: 15741.9053 - val_mse: 15741.9053 - val_mae: 97.2142\n",
      "Epoch 128/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0176 - mse: 0.0176 - mae: 0.0787 - val_loss: 15739.2510 - val_mse: 15739.2510 - val_mae: 97.2048\n",
      "Epoch 129/150\n",
      "13/13 [==============================] - 1s 95ms/step - loss: 0.0161 - mse: 0.0161 - mae: 0.0743 - val_loss: 15740.1807 - val_mse: 15740.1807 - val_mae: 97.2097\n",
      "Epoch 130/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0184 - mse: 0.0184 - mae: 0.0835 - val_loss: 15739.6631 - val_mse: 15739.6631 - val_mae: 97.2073\n",
      "Epoch 131/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0681 - val_loss: 15739.0322 - val_mse: 15739.0332 - val_mae: 97.2023\n",
      "Epoch 132/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0620 - val_loss: 15739.4150 - val_mse: 15739.4150 - val_mae: 97.2065\n",
      "Epoch 133/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0111 - mse: 0.0111 - mae: 0.0636 - val_loss: 15738.3672 - val_mse: 15738.3672 - val_mae: 97.2009\n",
      "Epoch 134/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0540 - val_loss: 15740.1846 - val_mse: 15740.1846 - val_mae: 97.2047\n",
      "Epoch 135/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0517 - val_loss: 15739.3232 - val_mse: 15739.3232 - val_mae: 97.2015\n",
      "Epoch 136/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0532 - val_loss: 15742.2129 - val_mse: 15742.2129 - val_mae: 97.2078\n",
      "Epoch 137/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.0079 - mse: 0.0079 - mae: 0.0558 - val_loss: 15740.0840 - val_mse: 15740.0840 - val_mae: 97.2046\n",
      "Epoch 138/150\n",
      "13/13 [==============================] - 1s 98ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0434 - val_loss: 15738.2412 - val_mse: 15738.2412 - val_mae: 97.2041\n",
      "Epoch 139/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0082 - mse: 0.0082 - mae: 0.0612 - val_loss: 15739.1973 - val_mse: 15739.1973 - val_mae: 97.2019\n",
      "Epoch 140/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0634 - val_loss: 15740.5332 - val_mse: 15740.5332 - val_mae: 97.2046\n",
      "Epoch 141/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0616 - val_loss: 15738.7305 - val_mse: 15738.7305 - val_mae: 97.1992\n",
      "Epoch 142/150\n",
      "13/13 [==============================] - 1s 94ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0499 - val_loss: 15738.8301 - val_mse: 15738.8301 - val_mae: 97.2014\n",
      "Epoch 143/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0499 - val_loss: 15739.4727 - val_mse: 15739.4727 - val_mae: 97.2001\n",
      "Epoch 144/150\n",
      "13/13 [==============================] - 1s 90ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0393 - val_loss: 15738.7852 - val_mse: 15738.7852 - val_mae: 97.2021\n",
      "Epoch 145/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0324 - val_loss: 15737.9756 - val_mse: 15737.9756 - val_mae: 97.1989\n",
      "Epoch 146/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0269 - val_loss: 15738.1055 - val_mse: 15738.1055 - val_mae: 97.2003\n",
      "Epoch 147/150\n",
      "13/13 [==============================] - 1s 89ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0296 - val_loss: 15739.5059 - val_mse: 15739.5059 - val_mae: 97.2006\n",
      "Epoch 148/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0364 - val_loss: 15738.6562 - val_mse: 15738.6562 - val_mae: 97.1993\n",
      "Epoch 149/150\n",
      "13/13 [==============================] - 1s 92ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0317 - val_loss: 15737.4141 - val_mse: 15737.4141 - val_mae: 97.2002\n",
      "Epoch 150/150\n",
      "13/13 [==============================] - 1s 97ms/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0302 - val_loss: 15739.0117 - val_mse: 15739.0117 - val_mae: 97.1995\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "history=model.fit(X_train, y_train, batch_size=80, epochs=150, verbose=1, validation_split=0.2)\n",
    "#predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mse', 'mae', 'val_loss', 'val_mse', 'val_mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tried to launch a AutoKeras model, iterating through 100 models trying to fine tune the hyper parameters. However, we thing that this technique is quite wrong since we select a model (and more particularly an epoch of the model) as being the best regarding the MSE on the validation set. This set being pretty small in our dataset (only 20% of the training set) we got super good results on the MSE of the model, however while testing on the testing set we created, it performed poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1997 :  2\n",
      " 1998 :  12\n",
      " 1999 :  31\n",
      " 2000 :  13\n",
      " 2001 :  29\n",
      " 2002 :  47\n",
      " 2003 :  45\n",
      " 2004 :  65\n",
      " 2005 :  66\n",
      " 2006 :  63\n",
      " 2007 :  81\n",
      " 2008 :  62\n",
      " 2009 :  96\n",
      " 2010 :  85\n",
      " 2011 :  95\n",
      " 2012 :  69\n",
      " 2013 :  59\n",
      " 2014 :  86\n",
      " 2015 :  78\n",
      " 2016 :  45\n",
      " 2017 :  52\n",
      " 2018 :  68\n",
      " 2019 :  36\n"
     ]
    }
   ],
   "source": [
    "### Gathering the data\n",
    "\n",
    "infile = open(\"C:/Users/hugop/Desktop/corpus-taln/corpus_taln_v1.tei.xml\",\"r\", encoding='UTF-8')\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents,'xml')\n",
    "\n",
    "soup.bibl.clear()\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "#### Scrapping\n",
    "\n",
    "#*We use the XML tags to retrieve information that we want, using beaufitul soup, since XML and HTML work the same way*\n",
    "\n",
    "#*authors*\n",
    "\n",
    "author = soup.find_all('text')\n",
    "\n",
    "selector = {'xml:lang':[\"fr\",\"en\"]}\n",
    "titles = soup.find_all('title', selector)\n",
    "\n",
    "title = []\n",
    "for x in titles:\n",
    "    title.append(x.text.replace('\\n',''))\n",
    "title = list(dict.fromkeys(title))\n",
    "\n",
    "titles_list = []\n",
    "for x in titles:\n",
    "    titles_list.append(str(x))\n",
    "\n",
    "titles_list = list(dict.fromkeys(titles_list))\n",
    "\n",
    "#*languages*\n",
    "\n",
    "language = []\n",
    "for x in titles_list:\n",
    "    language.append(re.search('\\\"(.*)\\\"', x).group(1))\n",
    "\n",
    "#*texts*\n",
    "\n",
    "paragraphs = []\n",
    "for x in author:\n",
    "    paragraphs.append(BeautifulSoup(str(x), \"lxml\").text.replace('\\n\\n\\n\\n\\n',' ').replace('\\n\\n\\n\\n',' ').replace('\\n\\n\\n',' ').replace('\\n\\n',' ').replace('\\n',' ').replace(\"\\'\",\"\").replace('None ','').replace('  ',' '))\n",
    "\n",
    "paragraphs_clean = []\n",
    "regex = re.compile(\".?((.?))\")\n",
    "for x in paragraphs:\n",
    "    paragraphs_clean.append(re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", x).replace('[]', '').replace('()', '').replace('  ', ' ').replace(' .','.'))\n",
    "paragraphs_clean = list(dict.fromkeys(paragraphs_clean))\n",
    "\n",
    "#*dates*\n",
    "\n",
    "dates = soup.find_all('imprint') #we use this tag because its unique in each article, avoids duplicates\n",
    "\n",
    "#*conference*\n",
    "\n",
    "conf = soup.find_all('fileDesc', {\"xml:id\": re.compile('^r|t')})\n",
    "\n",
    "conf = conf[1:] ##supprimer le fileDesc qui correspond à la description du fichier xml\n",
    "\n",
    "conferences = []\n",
    "for x in conf:\n",
    "    conferences.append(str(x).replace('-', '\"'))\n",
    "\n",
    "conferences_clean = []\n",
    "for x in conferences:\n",
    "    conferences_clean.append(re.search('\\\"(.*)\\\"', x).group(1))\n",
    "\n",
    "conferences_clean_2 = []\n",
    "for x in conferences_clean:\n",
    "    if x[0] == 't':\n",
    "        x = x[0:4]\n",
    "    else:\n",
    "        x = x[0:7]\n",
    "    conferences_clean_2.append(x)\n",
    "\n",
    "date = []\n",
    "for x in dates:\n",
    "    date.append(x.get_text().replace('\\n',''))\n",
    "\n",
    "#*Once we scrapped all the data we can zip the lists into a single pandas dataframe that we'll use later*\n",
    "\n",
    "#filling a dataframe with the scrapped data\n",
    "df = pd.DataFrame(list(zip(paragraphs_clean, language,title, date,conferences_clean_2)), columns =['Text', 'Language','name', 'date', 'conference']) \n",
    "\n",
    "df_fr = df[df['Language'] == 'fr'] #we focus on french papers only for this part\n",
    "\n",
    "df_fr = df_fr[df_fr['Text'].str.len() > 2570] #we delete the articles with no text body\n",
    "\n",
    "Counter(list(df_fr['conference']))\n",
    "\n",
    "#### Lowercasing the text\n",
    "\n",
    "df_fr['Text'] = df_fr['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df_fr['Text'].head()\n",
    "\n",
    "#### Removing the stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('french')\n",
    "df_fr['Text'] = df_fr['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df_fr['Text'].head()\n",
    "\n",
    "#### Removing the punctuation\n",
    "\n",
    "df_fr['Text'] = df_fr['Text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#### Lemmatization\n",
    "\n",
    "#### Stemming\n",
    "\n",
    "df_fr['Text_no_stemming'] = df_fr['Text']\n",
    "\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "stemmerFR = FrenchStemmer()\n",
    "df_fr['Text'] = df_fr.apply(lambda x: \" \".join([stemmerFR.stem(word) for word in x['Text'].split()]), axis = 1)\n",
    "\n",
    "df_fr['Text'].head()\n",
    "\n",
    "from collections import Counter\n",
    "sorted(list(df_fr['date']))\n",
    "\n",
    "def CountFrequency(my_list): \n",
    "  \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "  \n",
    "    for key, value in freq.items(): \n",
    "        print (\"% d : % d\"%(key, value)) \n",
    "\n",
    "CountFrequency(sorted(pd.to_numeric(df_fr['date'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df_fr = shuffle(df_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n",
      "cet articl présent appliqu doutil méthod trait aut\n"
     ]
    }
   ],
   "source": [
    "a = np.array(list(df_fr['Text']))\n",
    "\n",
    "X_train = a[:1000]\n",
    "y_train = np.array(list(df_fr['date']))[:1000]\n",
    "X_test = a[1000:]\n",
    "y_test = np.array(list(df_fr['date']))[1000:]\n",
    "print(X_train.shape)  # (25000,)\n",
    "print(y_train.shape)  # (25000, 1)\n",
    "print(X_train[0][:50])  # <START> this film was just brilliant casting <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for item in y_train:\n",
    "    a.append(float(item))\n",
    "\n",
    "\n",
    "y_train = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for item in y_test:\n",
    "    a.append(float(item))\n",
    "\n",
    "\n",
    "y_test = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'kerastuner' has no attribute 'engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-85d187d68977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# limitations under the License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertBlock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategoricalToNumerical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\auto_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgraph_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtuners\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\tuners\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# limitations under the License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesian_optimization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreedy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGreedy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperband\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHyperband\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\tuners\\bayesian_optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkerastuner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtuner\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtuner_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\engine\\tuner.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAutoTuner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkerastuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTuner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \"\"\"A Tuner class based on KerasTuner for AutoKeras.\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'kerastuner' has no attribute 'engine'"
     ]
    }
   ],
   "source": [
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'kerastuner' has no attribute 'engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-a8cc0dc78ecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Initialize the text regressor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m reg = ak.TextRegressor(\n\u001b[0;32m      5\u001b[0m     \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# limitations under the License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertBlock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCategoricalToNumerical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\auto_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgraph_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtuners\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\tuners\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# limitations under the License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesian_optimization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreedy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGreedy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuners\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperband\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHyperband\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\tuners\\bayesian_optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkerastuner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mautokeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtuner\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtuner_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\autokeras\\engine\\tuner.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAutoTuner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkerastuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTuner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \"\"\"A Tuner class based on KerasTuner for AutoKeras.\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'kerastuner' has no attribute 'engine'"
     ]
    }
   ],
   "source": [
    "import autokeras as ak\n",
    "\n",
    "# Initialize the text regressor.\n",
    "reg = ak.TextRegressor(\n",
    "    overwrite=True,\n",
    "    max_trials=10) # It tries 100 different models.\n",
    "# Feed the text regressor with training data.\n",
    "reg.fit(X_train, y_train, epochs=100)\n",
    "# Predict with the best model.\n",
    "predicted_y = reg.predict(X_test)\n",
    "# Evaluate the best model with testing data.\n",
    "print(reg.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reg.export_model()\n",
    "# summarize the loaded model\n",
    "model.summary()\n",
    "#model.save('model_reg.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for instance, we get the best model with 3183 of MSE while we only get 1744 on the testing set. We can see here that however this SME is pretty bad, it is highly imprecise. It is due to the low size of the training and testing sets sue to the small size of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1744.512867095805"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_pred, squared=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude this work, we can say that the TF-IDF outperformed the Doc2Vec approach, despite being pretty old. It's ability to filter the texts and the low complexity of the vectors it creates probably helps models to understand the relationships between dimensions (here words). It is particularly performant since we ran it on the whole texts, previously stemmed, giving the computed frequencies even more meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the prediction models, we can see that regression models seem to outperform classification model (even though it's hard to compare them) since they are able to model the years and not just different classes like a classification algorithm would do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we saw that models that require few data to train performed better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-7ddb8a3c1af0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mrf_random\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distributions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Fit the random search model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mrf_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1619\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1620\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1621\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[0;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
